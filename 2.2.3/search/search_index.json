{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Before to start","text":"<ul> <li> <p>If you don't know anything about glacier processes, explore this great website. If you don't know anything about glacier evolution modeling, you may watch first this video, which gives some basics. </p> </li> <li> <p>OS: IGM was developed in a Linux environment but works on Windows and Mac. Windows user are strongly recommended to use WSL2 for using the GPU and the OGGM shop module. </p> </li> <li> <p>Disclaimer: IGM implements empirical physical laws, with an important amount of approximations (of any kind). Make sure to understand what you do, to explore key parameters, and interpret the results with care.</p> </li> </ul>"},{"location":"#how-to-start","title":"How to start","text":"<p>Running IGM consists of running a python script <code>igm_run</code>, which is made of functions of the IGM python package. This documentation will help you to understand the parameters and, set-up your model by listing the modules you need, customize your own modules for your application.</p> <ul> <li> <p>Then, install an igm python environment on your system and starting with examples.</p> </li> <li> <p>Then, learn how to run IGM with module list and parameter setting (without extra coding), and explore the module documentation.</p> </li> <li> <p>Last, understand the code and write your own module code. </p> </li> </ul>"},{"location":"Installation/","title":"Installation","text":"<p>IGM is a Python package, which works on any OS on </p> <ul> <li>CPU (not computationally efficient, but fine for small applications like individual glaciers),</li> <li>GPU (the most computationally efficient way, especially relevant for large-scale and high-resolution applications). </li> </ul> <p>IGM can be installed with the </p> <ul> <li>the main version for stable application (the latest available tag), not all modules,</li> <li>the development version to get the latest feature with all modules (at the possible price of unrevealed bugs). </li> </ul> <p>Both versions are now on the same (main) branch. IGM is rapidly changing, keep track of updates on the release page for the tagged versions or/and on this page for the development version.</p> <p>Note that the igm package installs most of dependent packages, but not all. For using some post-processing modules, the user has to install additional packages (e.g., mayavi, plotly, ect.).</p> <p>We first describe the installation in Linux (the preferred OS), and then on Windows and Mac.</p>"},{"location":"Installation/#linux","title":"Linux","text":""},{"location":"Installation/#windows","title":"Windows","text":""},{"location":"Installation/#mac","title":"Mac","text":""},{"location":"Installation/#troubleshooting","title":"Troubleshooting","text":"<p>Main source of issues are linked to Tensorflow and the use of GPU. Hopefully, the installation is significantly easier since tensorflow 2.14.0 since it can install all necessary GPU/cuda dependent packages with the right version automatically. Note that to ensure smooth usage of GPU with cuda and tensorflow libraries, one has to make sure that i) cuda ii) cudnn iii) tensorflow are compatible, and your Nvidia driver is compatible with the version of cuda. Such incompatibility is the most common source of issue. </p> <p>For instance, it is possible do install tensorflow-2.12.0 by setting <code>tensorflow==2.12.0</code> in the setup.py and</p> <pre><code>conda install -c conda-forge cudatoolkit=11.8.0\npip install nvidia-cudnn-cu11==8.6.0.163\n\nmkdir -p ${CONDA_PREFIX}/etc/conda/activate.d\nD=${CONDA_PREFIX}/etc/conda/activate.d/env.sh\necho 'export PYTHONNOUSERSITE=1' &gt;&gt; $D\necho 'export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${CONDA_PREFIX}/lib' &gt;&gt; $D\necho 'export CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))' &gt;&gt; $D\necho 'export LD_LIBRARY_PATH=$CONDA_PREFIX/lib/:$CUDNN_PATH/lib:$LD_LIBRARY_PATH' &gt;&gt; $D\n</code></pre>"},{"location":"Running/","title":"How to Run IGM","text":"<p>Once IGM is installed, one can launch an experiment by using the following command:</p> <pre><code>igm_run +experiment=params\n</code></pre> <p>Importantly, before doing so you must</p> <ol> <li>Make sure the virtual environment where IGM is installed is activated.</li> <li>Verify that you are running this command in a folder where the <code>experiment</code> can be found.</li> <li>(Optional) If using custom modules, place them in the correct place.</li> </ol> <p>Assuming that you want to launch the <code>params</code> experiment, you can run the above command in the same level where the <code>experiment</code> folder belongs.</p> <pre><code>\u251c [RUN COMMAND HERE]\n\u251c\u2500\u2500 experiment  # contains the parameter file\n\u2502   \u2514\u2500\u2500 params.yaml\n</code></pre>"},{"location":"Running/#using-custom-modules","title":"Using custom modules","text":"<p>Optionally, if you have decided to include custom modules, you should also put these folders in the same level as the <code>experiment</code> folder (shown below). For more information on how to use custom modules in IGM, please visit here.</p> <pre><code>\u251c [RUN COMMAND HERE]\n\u251c\u2500\u2500 experiment  # contains the parameter file\n\u2502   \u2514\u2500\u2500 params.yaml\n\u2514\u2500\u2500 user        # contains user-modules if any\n\u2502   \u2514\u2500\u2500 conf\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 code\n\u2502       \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"about/FAQ/","title":"FAQ","text":""},{"location":"about/FAQ/#frequently-asked-questions-faq","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"about/FAQ/#ice-is-stuck-on-the-border-of-the-domain-no-fluxes-what-can-i-do","title":"Ice is stuck on the border of the domain (no fluxes), what can I do?","text":"<p>Set the parameter <code>exclude_borders_from_iceflow</code> to <code>True</code>.</p>"},{"location":"about/FAQ/#i-see-some-numerical-artifacts-eg-waves-occurring-when-modeling-glacier-evolution-what-can-i-do","title":"I see some numerical artifacts (e.g., waves) occurring when modeling glacier evolution, what can I do?","text":"<p>Reduce the <code>CFL</code> parameter.</p>"},{"location":"about/FAQ/#how-to-createmodify-netcdf-files","title":"How to create/modify NetCDF files?","text":"<p>There are many ways to prepare NetCDF files (e.g., MATLAB, Python, GIS tools). The NCO toolkit allows easy operations via command lines, for example:</p> <pre><code>ncks -x -v thk file.nc file.nc              # Removes the variable 'thk' from file.nc\nncks -v usurf file.nc file.nc               # Extracts the variable 'usurf' from file.nc\nncap2 -h -O -s 'thk=0*thk' file.nc file.nc  # Performs operations on file.nc, here forcing zero 'thk'\nncrename -v apc,strflowctrl file.nc         # Renames variable 'apc' to 'strflowctrl' in file.nc\n</code></pre>"},{"location":"about/FAQ/#oggm-shop-produces-an-error-on-windows","title":"OGGM Shop produces an error on Windows","text":"<p>This is expected, as OGGM is not supported on Windows. However, modifying the <code>tarfile.py</code> file at line 2677 from <code>name == member_name</code> to <code>name.replace(os.sep, '/') == member_name</code> seems to fix the issue on Windows. Thanks to Alexi Morin for proposing this workaround.</p>"},{"location":"about/FAQ/#gpu-vs-cpu","title":"GPU vs CPU","text":"<p>IGM works fine on CPUs for small computational domains (typically individual glaciers). In contrast, GPUs are highly advantageous for very large computational grids (e.g., large networks of glaciers), as IGM naturally benefits from parallelism. See this example.</p>"},{"location":"about/credits_and_references/","title":"Credits","text":""},{"location":"about/credits_and_references/#main-developpers","title":"Main developpers","text":"Name Contributions Guillaume Jouvet, UNIL Primary source code author of most of modules and their documentation, design of the PI-CNN in forward [1,3] and inverse modes [2]. Brandon Finley, UNIL Core code and software design, hydra integration, code profiling, docker, documentation framework design, <code>texture</code> module, TBC"},{"location":"about/credits_and_references/#contributors","title":"Contributors","text":"<p>(get in touch if you notice any missing contribution)</p> Name Contributions Flavio Calvo Support from the release IGM1 to IGM2 Samuel Cook Implementation of functions specific for global modelling in the <code>data_assimilation</code> module, inclusion of RGI7 in <code>oggm_shop</code> Guillaume Cordonnier Co-design of the PI-CNN in forward mode, original implementation of the <code>thk</code> module Alex Jarosch Bueler2005C's benchmark case in the example gallery Andreas Henz Loading icemasks from shape files in input modules Tancr\u00e8de Leger Climate module <code>XXXX</code> Fabien Maussion Original <code>oggm_shop</code> module, and support for the integration of OGGM-based <code>clim_oggm</code> and <code>smb_oggm</code> modules, and instructed OGGM routine. J\u00fcrgen Mey <code>avalanche</code> and <code>glex</code> modules Oskar Hermann Vizualization tool <code>anim_plotly</code> Dirk Scherler Support for the verication of the <code>particle</code> module Margot Sirdey Support from the release IGM1 to IGM2, PiPy setup Gillian Smith Bug fixes Patrick Schmitt Vizualization tool, TBC Claire-Mathile St\u00fccki Update of <code>vert_flow</code> and <code>particle</code> modules Ethan Welthy GlaThiDa file reading"},{"location":"about/credits_and_references/#citing-igm","title":"Citing IGM","text":"<p>The foundational concepts of IGM are as follows: The modeling approach using data-driven ice flow convolutional neural networks (CNN) was introduced in [3], the inversion method was introduced in [2], and the physics-informed ice flow surrogate neural network (SNN) was introduced in [13]. There is currently an in-progress IGM technical paper that provides an overview of the physical components, modules, and capabilities of IGM. Until the technical paper is finalized, [1] serves as the most up-to-date reference for understanding IGM concepts, and should therefore be used for referencing IGM.</p> <p>[1] Jouvet, G., &amp; Cordonnier, G. (2023). Ice-flow model emulator based on physics-informed deep learning. Journal of Glaciology, 69(278), 1941-1955.</p> <p>[2] Jouvet, G. (2023). Inversion of a Stokes glacier flow model emulated by deep learning. Journal of Glaciology, 69(273), 13-26.</p> <p>[3] Jouvet, G., Cordonnier, G., Kim, B., L\u00fcthi, M., Vieli, A., &amp; Aschwanden, A. (2022). Deep learning speeds up ice flow modelling by several orders of magnitude. Journal of Glaciology, 68(270), 651-664.</p>"},{"location":"about/credits_and_references/#bibtex-entries","title":"Bibtex entries","text":"<p><pre><code>@article{IGM,\n    author       = \"Jouvet, Guillaume and Cordonnier, Guillaume and Kim, Byungsoo and L\u00fcthi, Martin and Vieli, Andreas and Aschwanden, Andy\",  \n    title        = \"Deep learning speeds up ice flow modelling by several orders of magnitude\",\n    DOI          = \"10.1017/jog.2021.120\",\n    journal      = \"Journal of Glaciology\",\n    year         =  2021,\n    pages        = \"1\u201314\",\n    publisher    = \"Cambridge University Press\"\n}\n</code></pre> <pre><code>@article{IGM-inv,\n    author       = \"Jouvet, Guillaume\",\n    title        = \"Inversion of a Stokes ice flow model emulated by deep learning\",\n    DOI          = \"10.1017/jog.2022.41\",\n    journal      = \"Journal of Glaciology\",\n    year         = \"2022\",\n    pages        = \"1--14\",\n    publisher    = \"Cambridge University Press\"\n}\n</code></pre> <pre><code>@article{IGM-PINN,\n    title={Ice-flow model emulator based on physics-informed deep learning},\n    author={Jouvet, Guillaume and Cordonnier, Guillaume},\n    journal={Journal of Glaciology},\n    pages={1--15},\n    year={2023},\n    publisher={Cambridge University Press},\n    doi={10.1017/jog.2023.73}\n}\n</code></pre> <pre><code>@Misc{Yadan2019Hydra,\n  author =       {Omry Yadan},\n  title =        {Hydra - A framework for elegantly configuring complex applications},\n  howpublished = {Github},\n  year =         {2019},\n  url =          {https://github.com/facebookresearch/hydra}\n}\n</code></pre></p>"},{"location":"about/help/","title":"Need Help?","text":""},{"location":"about/help/#discord-chat-server","title":"Discord Chat Server","text":"<p>For any IGM-related questions, feel free to connect with the community on the Discord channel. To gain access, send an email request to guillaume.jouvet@unil.ch.</p>"},{"location":"about/help/#support-guidelines","title":"Support Guidelines","text":"<p>If you require assistance or feedback from developers or other users, and you are encountering an issue, it is essential to share a fully reproducible and well-organized setup folder. This will streamline communication and save time. Please provide a download link or even better an access to a github repository with a setup folder structured as follows:</p> <ul> <li>A sub-folder (e.g., <code>experiment</code>) containing the parameter file.</li> <li>A sub-folder (e.g., <code>data</code>) containing the input data (if applicable).</li> <li>A sub-folder (e.g., <code>user</code>) containing user modules (if applicable).</li> <li>A ReadMe file that includes<ol> <li>The exact version of IGM (tag or version to check out).</li> <li>The command to run to reproduce the error (e.g., <code>igm_run experiment=params</code>).</li> <li>A clear description of the issue.</li> <li>(Optional) The version of external libraries, if relevant (e.g., TensorFlow, xarray, etc.).</li> </ol> </li> </ul>"},{"location":"about/tools/","title":"IGM-Related Tools","text":"<p>Here are some tools we recommend for handling data, coding, visualizing results, etc. These tools integrate well with the IGM ecosystem.</p>"},{"location":"about/tools/#data-handling","title":"Data Handling","text":"<p>Unless using the <code>oggm_shop</code>, you need to prepare data for IGM. IGM preferably takes NetCDF files. Below are some tools to handle NetCDF (or TIFF) files:</p> <ul> <li>ncview: A lightweight utility for quick visualization of NetCDF files.</li> <li>nco: A suite of command-line tools for performing operations on NetCDF files. You can also produce or modify NetCDF files using independent Python scripts.</li> <li>gdal: A powerful library and set of tools for working with TIFF files (and NetCDF files as well).</li> </ul>"},{"location":"about/tools/#editor-vs-code","title":"Editor: VS Code","text":"<p>VS Code is an excellent editor and is highly recommended for working with IGM. It offers features such as: - Syntax highlighting for Python, YAML, and other languages. - Integration with AI tools like Copilot and OpenAI. - Remote server connection for coding and running IGM remotely. - Powerful extensions, such as H5Web, for visualizing NetCDF files.</p>"},{"location":"about/tools/#visualization","title":"Visualization","text":"<ul> <li> <p><code>utils/anim_plotly.py</code>: Enables interactive 3D visualization of IGM results by reading NetCDF files. It uses the <code>dash</code> and <code>plotly</code> libraries (<code>pip install dash plotly</code>). This script creates a Dash app accessible via a browser (usually at <code>http://127.0.0.1:8050/</code>). The app displays a 3D plot of the glacier's surface over the surrounding bedrock. Surface color can represent ice thickness, velocity magnitude, or surface mass balance. Variables can be selected from a dropdown menu, and a slider allows navigation through different time steps. This tool was implemented by Oskar Herrmann.</p> </li> <li> <p>Glacier:3D-Viz tool is a visualization tool developed by the OGGM team (Patrick Schmitt) to create 3D visualizations of changing glaciers. It is primarily built on the <code>PyVista</code> package. Glacier:3D-Viz can read IGM-like output data. Refer to their documentation for more information.</p> </li> <li> <p><code>utils/anim_mayavi.py</code>: Creates a 3D animated plot from the NetCDF output (default <code>output.nc</code>) file produced by IGM. This module depends on the <code>mayavi</code> and <code>pyqt5</code> libraries (<code>pip install mayavi pyqt5</code>). Note: This module works only with Python versions &lt;= 3.10.</p> </li> <li> <p><code>utils/anim_video.py</code>: Generates an animated MP4 video of ice thickness over time from the NetCDF output (default <code>output.nc</code>) file produced by IGM. This module depends on the <code>xarray</code> library.</p> </li> <li> <p><code>utils/make_film.py</code>: A utility for creating MP4 films from a set of images (developed by T. Leger).</p> </li> </ul>"},{"location":"about/transition-IGM-2-to-3/","title":"Transitioning IGM from v2 to v3: Guideline","text":""},{"location":"about/transition-IGM-2-to-3/#in-short","title":"In short","text":"<p>This is a major structural update, and we are starting a new versioning series with IGM 3.0.0. Below are the main changes and improvements you can expect:</p> <ul> <li> <p>Hydra integration (game changer!): The most important update is the integration of the Hydra library for handling parameters. This allows you to run large multi-ensemble simulations (poss on multiple GPUs) from a single command line, with full traceability and reproducibility. No more manually managing complex parameter files\u2014Hydra does it for you, and does it extremely well. This alone is a compelling reason to switch to IGM3!</p> </li> <li> <p>YAML-based parameters with hierarchical structure: All parameters are now defined in YAML (instead of JSON), offering a more powerful and flexible configuration format. Especially, parameters are now organized hierarchically, making it much easier to manage large configurations. This comes with renamed parameters, but don\u2019t worry\u2014we provide a tool to convert old JSON files to the new YAML format, along with a conversion table.</p> </li> <li> <p>New documentation website: A freshly built documentation website now automatically collects all parameter definitions, along with their default values, descriptions, and units.</p> </li> <li> <p>Clean folder structure: The working directory is now properly organized into subfolders for parameters, data, user modules, and output.</p> </li> <li> <p>Improved code readability: Key modules like iceflow and data_assimilation (formerly optimize) have been split into sub-files to improve readability, maintainability, and customization.</p> </li> </ul> <p>Here are the important new links:</p> <ul> <li> <p>The pre-release is on the usual https://github.com/jouvetg/igm , but you need to checkout the feature/hydra branch.</p> </li> <li> <p>The new documentation website is here : https://jouvetg.github.io/igm-doc/</p> </li> <li> <p>The separate repo containing examples : https://github.com/instructed-glacier-model/igm-examples</p> </li> <li> <p>The repo with the current technical paper re-shaped for IGM 3.0.0 : https://github.com/instructed-glacier-model/igm-paper</p> </li> </ul>"},{"location":"about/transition-IGM-2-to-3/#in-more-details-for-users","title":"In more details (for users)","text":""},{"location":"about/transition-IGM-2-to-3/#parameter-handling","title":"Parameter handling","text":""},{"location":"about/transition-IGM-2-to-3/#hydra-library-replaces-parser-and-yaml-file","title":"<code>hydra</code> library replaces <code>parser</code>, and YAML file","text":"<p>Parameters previously managed with the <code>parser</code> library are now handled using the <code>hydra</code> library. The parameter file, which was formerly a JSON file (<code>params.json</code>), has been transitioned to a YAML file (<code>experiment/params.yaml</code>) adhering to the YAML standard. This shift to <code>hydra</code> has significantly simplified the core IGM code. Running IGM now involves executing commands like:</p> <pre><code> igm_run +experiment=params \n</code></pre> <p>where parameters can be changed within the <code>params.yaml</code> file, or overridden directly in the command line. For example:</p> <pre><code>igm_run +experiment=params processes.time.start=1900 processes.time.end=2100\n</code></pre> <p>From a user perspective, migrating to IGM 3 essentially involves converting the former parameter file <code>params.json</code> into the new <code>experiment/params.yaml</code> format. To facilitate this transition, we provide a utility script named <code>json_to_yaml.py</code> (in the root of IGM repo) that automates the conversion process (you may have to adjust manually, check it afterwards!).</p>"},{"location":"about/transition-IGM-2-to-3/#parameter-naming-changes-and-hierarchical-construction","title":"Parameter Naming Changes, and hierarchical construction","text":"<p>Parameter names have been slightly modified. Previously, all parameters included a prefix like <code>iflo</code> to indicate they were associated with the <code>iceflow</code> module. Now, parameters are organized hierarchically by attributes, making the prefix redundant and therefore removed. For example: <code>time_start</code> is now accessible as <code>processes.time.start</code>.</p> <p>In the <code>iceflow</code> module, parameters have been significantly reorganized to follow a hierarchical structure. To assist with the transition, we provide a script named <code>json_to_yaml.py</code>, and correspondence table to help convert old JSON files into the new YAML format. Also, IGM now raises an error if a parameter contains a typo, and suggests parameters to pick.</p>"},{"location":"about/transition-IGM-2-to-3/#example-of-new-parameter-file","title":"Example of new parameter file","text":"<p>Here is what the new parameter file looks like:</p> <pre><code># @package _global_\n\ncore:\n  url_data: \"https://www.dropbox.com/scl/fo/8ixpy27i67s04bp7uixoq/h?rlkey=0ye7rd4zkcqfhvzx7suunw3bk&amp;dl=0\"\n\ndefaults:\n  - override /inputs: [local]\n  - override /processes: [smb_simple, iceflow, time, thk]\n  - override /outputs: [local, plot2d]\n\ninputs:\n  local:\n    input_file: input.nc\n\nprocesses:\n  iceflow:\n    physics:\n      init_slidingco: 0.0595\n  time:\n    start: 1880.0\n    end: 2020.0\n    save: 5.0 \n\noutputs:\n  plot2d:\n    live: true\n</code></pre> <p>The above file will be read by Hydra in IGM and has the following structure:</p> <ul> <li><code>core</code>: Includes all parameters specific to the core IGM run, such as logging, downloading data prior to the run, GPU-related settings, etc.</li> <li><code>defaults</code>: Lists the input, process, and output modules to be used.</li> <li><code>inputs</code>: Contains parameters to override the defaults for input modules.</li> <li><code>processes</code>: Contains parameters to override the defaults for process modules.</li> <li><code>outputs</code>: Contains parameters to override the defaults for output modules.</li> </ul> <p>Note: The former module types <code>preprocess</code>, <code>process</code>, and <code>postprocess</code> have been renamed to <code>input</code>, <code>modules</code>, and <code>output</code>, respectively. </p>"},{"location":"about/transition-IGM-2-to-3/#running-multiple-runs","title":"Running multiple runs","text":"<p>A great advantage of Hydra is the possibility it provides for running ensemble simulations. For example, the following command sequentially runs two simulations with two different sets of parameters:</p> <pre><code>igm_run -m +experiment=params processes.time.end=2080,2110\n</code></pre> <p>Alternatively, if you supply two parameter files (<code>params1</code> and <code>params2</code>), you can execute:</p> <pre><code>igm_run -m +experiment=params1,params2\n</code></pre> <p>You can also perform a grid search over multiple parameters. For instance, the following command runs simulations for a 3x2 parameter grid:</p> <pre><code>igm_run -m +experiment=params processes.time.start=1900,1910,1920 processes.time.end=2080,2110\n</code></pre> <p>In such cases, the output folder will be named <code>multirun</code> instead of <code>output</code>.</p>"},{"location":"about/transition-IGM-2-to-3/#new-structure-of-working-folder","title":"New structure of working folder","text":"<p>The working folder is structured as follows:</p> <ul> <li><code>experiment</code>: Contains the parameter files.</li> <li><code>data</code>: Stores input data, if any.</li> <li><code>user</code>: Contains user-defined/custom Python functions or modules.</li> <li><code>output</code> or <code>multirun</code>: Contains the results of the IGM runs.</li> </ul> <p>The folder structure looks like this:</p> <pre><code>\u251c\u2500\u2500 experiment\n\u2502   \u2514\u2500\u2500 params.yaml\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 user\n\u2502   \u251c\u2500\u2500 code\n\u2502   \u2502   \u2514\u2500\u2500 processes\n\u2502   \u2502       \u2514\u2500\u2500 mymodule.py\n\u2502   \u2514\u2500\u2500 conf\n\u2502       \u2514\u2500\u2500 processes\n\u2502           \u2514\u2500\u2500 mymodule.yaml\n\u2514\u2500\u2500 output\n  \u251c\u2500\u2500 2025-03-06\n  \u2502   \u2514\u2500\u2500 15-43-37\n  \u2502   \u2514\u2500\u2500 15-44-07\n  \u2502       \u2514\u2500\u2500 output.nc\n  \u2502       \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"about/transition-IGM-2-to-3/#integration-and-exclusion-of-former-modules","title":"Integration and Exclusion of Former Modules","text":"<ul> <li> <p>Splitting of <code>iceflow</code> modules into <code>data_assimilation</code> and <code>pretraining</code>: The <code>iceflow</code> module has been split into two dedicated modules: <code>data_assimilation</code> (formerly <code>optimize</code>) and <code>pretraining</code>. This change was made to improve code organization and maintainability, as the <code>iceflow</code> module had grown too large. Previously, <code>data_assimilation</code> and <code>pretraining</code> were separate but had dependency issues with <code>iceflow</code>. With the new structure, both modules remain dependent on <code>iceflow</code>, but these dependencies are now handled seamlessly. When using <code>data_assimilation</code> or <code>pretraining</code>, ensure that the <code>iceflow</code> module is also included in your configuration. The order of inclusion does not matter.</p> </li> <li> <p>The modules <code>anim_mayavi</code>, <code>anim_plotly</code>, and <code>anim_video</code> have been externalized from IGM and moved to <code>utils</code>. These modules were purely for postprocessing and were executed at the very end. To simplify the core structure, they were externalized.</p> </li> <li> <p>Modules such as <code>print_comp</code> and <code>print_info</code> have been integrated into the core functionality of IGM.</p> </li> <li> <p>The <code>write_particles</code> functionality is now integrated into the <code>particule</code> module.</p> </li> </ul>"},{"location":"about/transition-IGM-2-to-3/#new-local-module-merging-netcdf-and-tif","title":"New <code>local</code> module merging netcdf and tif","text":"<p>A new I/O module, <code>local</code>, has been introduced to replace the <code>load_XXX</code> and <code>write_XXX</code> modules. The <code>local</code> module leverages the <code>xarray</code> library, which is more powerful and supports loading both NetCDF (<code>.nc</code>) and GeoTIFF (<code>.tif</code>) files.</p>"},{"location":"about/transition-IGM-2-to-3/#oggm_shop-requires-coupling-with-load_ncdf-or-local","title":"<code>oggm_shop</code> requires coupling with <code>load_ncdf</code> or <code>local</code>","text":"<p>The <code>oggm_shop</code> module now exclusively handles downloading data (e.g., RGIXXXX folders) using OGGM and converting it into a NetCDF file (<code>input.nc</code>) that adheres to IGM's naming conventions. However, it no longer performs the task of loading this data into IGM. To process the downloaded data, you must pair <code>oggm_shop</code> with either the <code>load_ncdf</code> or <code>local</code> modules. </p> <p>For example, if you use <code>oggm_shop</code>, you must include <code>load_ncdf</code> or <code>local</code> as additional <code>inputs</code> modules in your configuration.</p>"},{"location":"about/transition-IGM-2-to-3/#default-parameter-value-changes-upcoming-release","title":"Default Parameter Value Changes (Upcoming Release)","text":"<p>In the upcoming release, several default parameter values will be updated to improve performance and usability. These changes are not applied yet but will be included in the next version:</p> <ul> <li> <p><code>retrain_freq</code> (current default: 10): The current default retrains the ice flow CNN every 10 time steps. This frequency has been found insufficient in many cases. The next release will increase the retraining frequency to every 3 time steps.</p> </li> <li> <p><code>smooth_anisotropy_factor</code> (current default: 0.2): The current value of 0.2 for anisotropic smoothing has been observed to cause chessboard effects in ice thickness. To address this, the next release will safely deactivate anisotropic smoothing by setting this parameter to 1.0.</p> </li> <li> <p><code>convexity_weight</code> (current default: unknown): The current convexity weight, which is particularly useful in the absence of data, has been found to be highly empirical and non-intuitive. As a result, it will be deactivated in the next release by setting its value to 0.</p> </li> <li> <p><code>fix_opti_normalization_issue</code> (current default: false): A normalization issue has been identified in the current cost function for data assimilation, where some terms use sums while others use means. This inconsistency has been compensated for by adjusting regularization parameters. The next release will address this issue by setting this parameter to <code>true</code>, ensuring consistent use of means throughout. Consequently, users will need to significantly increase the regularization parameters (approximately \\(10^3\\) for thickness and \\(10^{10}\\) for sliding coefficients).</p> </li> <li> <p><code>log_slidingco</code> (current default: false): In the next release, the optimization will handle the square root of <code>slidingco</code> (scaled) instead of <code>slidingco</code> directly. This approach ensures that <code>slidingco</code> remains positive without explicitly enforcing positivity. Note that this change will affect the scaling of <code>slidingco</code> (approximately \\(10^{-6}\\)) when this option is set to <code>true</code>.</p> </li> <li> <p>`lr_init`` (current default: 0.0001): In the next release, it will be reduced to 0.001 to make the training more agressive.</p> </li> </ul>"},{"location":"about/transition-IGM-2-to-3/#in-more-details-for-developpers","title":"In more details (for developpers)","text":""},{"location":"about/transition-IGM-2-to-3/#hierachical-and-separated-parameters-and-code","title":"Hierachical and separated parameters and code","text":"<p>In IGM source code, the parameters and the code are now hierarchically organized. Default parameters are stored in the <code>igm/igm/conf</code> folder, which is separate from the code located in the <code>igm/igm</code> folder. This separation ensures a clean organization of configuration files and source code. Parameters are grouped into logical categories and subcategories within the YAML files. This structure mirrors the organization of the modules and processes in IGM. For example, the folder structure for default parameters looks like this:</p> <pre><code>igm/igm/conf\n\u251c\u2500\u2500 inputs\n\u2502   \u2514\u2500\u2500 local.yaml\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 processes\n\u2502   \u2514\u2500\u2500 iceflow.yaml\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 outputs\n  \u2514\u2500\u2500 plot2d.yaml\n  \u2514\u2500\u2500 ...\n</code></pre> <p>In the code, all parameters are accessible through the object <code>cfg</code>. For example, <code>cfg.processes.enthalpy.ref_temp</code> refers to a parameter associated with the <code>enthalpy</code> processes module.</p> <p>On the other hand the structure for the code looks very similar:</p> <pre><code>igm/igm\n\u251c\u2500\u2500 inputs\n\u2502   \u2514\u2500\u2500 local\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 processes\n\u2502   \u2514\u2500\u2500 iceflow\n\u2502   \u2514\u2500\u2500 avalanche\n\u2514\u2500\u2500 outputs\n  \u2514\u2500\u2500 plot2d\n  \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"about/transition-IGM-2-to-3/#custom-modules-now-called-user","title":"Custom modules (now called \"user\")","text":"<p>User modules are very useful when customizing applications. They can be used to tailor input, process, or output methods. To create such a user module, you need to create (or update) the <code>user</code> folder located at the root of your working directory, ensuring the following hierarchy is respected:</p> <pre><code>\u2514\u2500\u2500 user\n    \u251c\u2500\u2500 code\n    \u2502   \u2514\u2500\u2500 inputs \n    \u2502   \u2502   \u2514\u2500\u2500 my_inputs_module\n    \u2502   \u2502       \u2514\u2500\u2500 my_inputs_module.py \n    \u2502   \u2514\u2500\u2500 processes \n    \u2502   \u2502   \u2514\u2500\u2500 my_processes_module\n    \u2502   \u2502       \u2514\u2500\u2500 my_processes_module.py \n    \u2502   \u2514\u2500\u2500 outputs \n    \u2502       \u2514\u2500\u2500 my_outputs_module\n    \u2502           \u2514\u2500\u2500 my_outputs_module.py \n    \u2514\u2500\u2500 conf\n        \u2514\u2500\u2500 inputs\n            \u2514\u2500\u2500 my_inputs_module.yaml \n        \u2514\u2500\u2500 processes\n            \u2514\u2500\u2500 my_processes_module.yaml \n        \u2514\u2500\u2500 outputs\n            \u2514\u2500\u2500 my_outputs_module.yaml \n</code></pre> <p>where <code>my_processes_module.py</code> has the following structure (and require to define function <code>initialize</code>, <code>update</code>, and <code>finalize</code>):</p> <pre><code>def initialize(cfg,state):\n  ... \n\ndef update(cfg,state):\n  cfg.processes.clim_aletsch.time_resolution\n  ... \n\ndef finalize(cfg,state):\n  pass\n</code></pre> <p>Note that <code>my_inputs_module.py</code> and <code>my_outputs_module.py</code> only require function <code>run</code> (and <code>initialize</code> for output).</p> <p>Parameter files located in <code>conf/inputs</code>, <code>conf/processes</code>, and <code>conf/outputs</code> look like  <pre><code>update_freq: 1\ntime_resolution: 365\n</code></pre> or in case there is no parameter (the file must exist as follows even if no parameter are defined): <pre><code>null\n</code></pre></p> <p>It is important to note that user modules take precedence over official modules. If a user module shares the same name as an official module, the user module will override the official one, and the official module will be ignored.</p>"},{"location":"about/transition-IGM-2-to-3/#how-to-transition-your-own-user-modules","title":"How to transition your own user modules?","text":"<ul> <li> <p>Parameter changes: Update your user module code to replace each parameter reference (e.g., <code>params.iflo_exp_glen</code>) with the new naming convention (e.g., <code>cfg.processes.iceflow.physics.exp_glen</code>) as outlined in the parameter change tables below.</p> </li> <li> <p>Function updates: Modify the function argument for <code>initialize</code>, <code>update</code>, and <code>finalize</code> to accept <code>cfg</code> as an argument instead of <code>params</code>. The <code>state</code> argument remains unchanged.</p> </li> <li> <p>Own parameter: Remove the <code>params(parser)</code> function that previously defined your parameter list. Instead, transfer your parameter definitions into the appropriate YAML files within the <code>conf</code> folder, following the structure described in the \"Custom modules (now called 'user')\" section.</p> </li> </ul>"},{"location":"about/transition-IGM-2-to-3/#complex-modules-made-more-readables-and-more-customizable","title":"Complex modules made more readables, and more customizable","text":"<p>Key modules like <code>iceflow</code> and <code>data_assimilation</code> (formerly optimize) have been split into sub-files to improve readability, maintainability, and customization. Sometimes, you may need to modify an existing built-in module, e.g. to test a new feature in the iceflow emulator/solver, or new cost in the data assimilation. This can be achieved by creating a user module that overrides the built-in functionality. Check at the page on <code>user modules</code> in the documentation.</p>"},{"location":"about/transition-IGM-2-to-3/#parameter-changes","title":"Parameter changes","text":""},{"location":"about/transition-IGM-2-to-3/#parameter-name-change-table-for-iceflow-module","title":"Parameter name change table for <code>iceflow</code> module","text":"Formerly New name iflo_type iceflow.method iflo_force_max_velbar iceflow.force_max_velbar iflo_gravity_cst iceflow.physics.gravity_cst iflo_ice_density iceflow.physics.ice_density iflo_init_slidingco iceflow.physics.init_slidingco iflo_init_arrhenius iceflow.physics.init_arrhenius iflo_enhancement_factor iceflow.physics.enhancement_factor iflo_exp_glen iceflow.physics.exp_glen iflo_exp_weertman iceflow.physics.exp_weertman iflo_regu_glen iceflow.physics.regu_glen iflo_regu_weertman iceflow.physics.regu_weertman iflo_new_friction_param iceflow.physics.new_friction_param iflo_dim_arrhenius iceflow.physics.dim_arrhenius iflo_regu iceflow.physics.regu iflo_thr_ice_thk iceflow.physics.thr_ice_thk iflo_min_sr iceflow.physics.min_sr iflo_max_sr iceflow.physics.max_sr iflo_force_negative_gravitational_energy iceflow.physics.force_negative_gravitational_energy iflo_cf_eswn iceflow.physics.cf_eswn iflo_cf_cond iceflow.physics.cf_cond iflo_Nz iceflow.numerics.Nz iflo_vert_spacing iceflow.numerics.vert_spacing iflo_solve_step_size iceflow.solver.step_size iflo_solve_nbitmax iceflow.solver.nbitmax iflo_solve_stop_if_no_decrease iceflow.solver.stop_if_no_decrease iflo_optimizer_solver iceflow.solver.optimizer iflo_optimizer_lbfgs iceflow.solver.lbfgs iflo_save_cost_solver iceflow.solver.save_cost iflo_fieldin iceflow.emulator.fieldin iflo_retrain_emulator_freq iceflow.emulator.retrain_freq iflo_retrain_emulator_lr iceflow.emulator.lr iflo_retrain_emulator_lr_init iceflow.emulator.lr_init iflo_retrain_warm_up_it iceflow.emulator.warm_up_it iflo_retrain_emulator_nbit_init iceflow.emulator.nbit_init iflo_retrain_emulator_nbit iceflow.emulator.nbit iflo_retrain_emulator_framesizemax iceflow.emulator.framesizemax iflo_pretrained_emulator iceflow.emulator.pretrained iflo_emulator iceflow.emulator.name iflo_save_model iceflow.emulator.save_model iflo_exclude_borders iceflow.emulator.exclude_borders iflo_optimizer_emulator iceflow.emulator.optimizer iflo_optimizer_emulator_clipnorm iceflow.emulator.optimizer_clipnorm iflo_optimizer_emulator_epsilon iceflow.emulator.optimizer_epsilon iflo_save_cost_emulator iceflow.emulator.save_cost iflo_output_directory iceflow.emulator.output_directory iflo_network iceflow.emulator.network.architecture iflo_multiple_window_size iceflow.emulator.network.multiple_window_size iflo_activation iceflow.emulator.network.activation iflo_nb_layers iceflow.emulator.network.nb_layers iflo_nb_blocks iceflow.emulator.network.nb_blocks iflo_nb_out_filter iceflow.emulator.network.nb_out_filter iflo_conv_ker_size iceflow.emulator.network.conv_ker_size iflo_dropout_rate iceflow.emulator.network.dropout_rate iflo_weight_initialization iceflow.emulator.network.weight_initialization"},{"location":"about/transition-IGM-2-to-3/#parameter-name-change-table-for-optimize-data_assimilation-module","title":"Parameter name change table for <code>optimize</code> (<code>data_assimilation</code>) module","text":"Formerly New name opti_control data_assimilation.control_list opti_cost data_assimilation.cost_list opti_nbitmin data_assimilation.optimization.nbitmin opti_nbitmax data_assimilation.optimization.nbitmax opti_step_size data_assimilation.optimization.step_size opti_step_size_decay data_assimilation.optimization.step_size_decay opti_init_zero_thk data_assimilation.optimization.init_zero_thk opti_sole_mask data_assimilation.optimization.sole_mask opti_retrain_iceflow_model data_assimilation.optimization.retrain_iceflow_model opti_fix_opti_normalization_issue data_assimilation.optimization.fix_opti_normalization_issue opti_regu_param_thk data_assimilation.regularization.thk opti_regu_param_slidingco data_assimilation.regularization.slidingco opti_regu_param_arrhenius data_assimilation.regularization.arrhenius opti_regu_param_div data_assimilation.regularization.divflux opti_smooth_anisotropy_factor data_assimilation.regularization.smooth_anisotropy_factor opti_smooth_anisotropy_factor_sl data_assimilation.regularization.smooth_anisotropy_factor_sl opti_convexity_weight data_assimilation.regularization.convexity_weight opti_convexity_power data_assimilation.regularization.convexity_power opti_to_regularize data_assimilation.regularization.to_regularize opti_usurfobs_std data_assimilation.fitting.usurfobs_std opti_velsurfobs_std data_assimilation.fitting.velsurfobs_std opti_thkobs_std data_assimilation.fitting.thkobs_std opti_divfluxobs_std data_assimilation.fitting.divfluxobs_std opti_uniformize_thkobs data_assimilation.fitting.uniformize_thkobs opti_include_low_speed_term data_assimilation.fitting.include_low_speed_term opti_velsurfobs_thr data_assimilation.fitting.velsurfobs_thr opti_log_slidingco data_assimilation.fitting.log_slidingco opti_divflux_method data_assimilation.divflux.method opti_force_zero_sum_divflux data_assimilation.divflux.force_zero_sum opti_scaling_thk data_assimilation.scaling.thk opti_scaling_usurf data_assimilation.scaling.usurf opti_scaling_slidingco data_assimilation.scaling.slidingco opti_scaling_arrhenius data_assimilation.scaling.arrhenius opti_output_freq data_assimilation.output.freq opti_plot2d_live data_assimilation.output.plot2d_live opti_plot2d data_assimilation.output.plot2d opti_save_result_in_ncdf data_assimilation.output.save_result_in_ncdf opti_save_iterat_in_ncdf data_assimilation.output.save_iterat_in_ncdf opti_editor_plot2d data_assimilation.output.editor_plot2d opti_vars_to_save data_assimilation.output.vars_to_save opti_infer_params data_assimilation.cook.infer_params opti_tidewater_glacier data_assimilation.cook.tidewater_glacier opti_vol_std data_assimilation.cook.vol_std"},{"location":"about/variables/","title":"Variables","text":"<p>In general, IGM adopts name convention of PISM. Variable names serves to identify when outputing them, or in the code, they can be accessed (read and write) by <code>state.varname</code>. Here is a minimal list of key variables:</p> Variable names Shape Description Unit t () Time variable (scalar) \\(y\\) dt () Time step (scalar) \\(y\\) x,y (nx) Coordinates vectors \\(m\\) thk (ny) Ice thickness \\(m\\) topg (ny,nx) Basal topography (or bedrock) \\(m\\) usurf (ny,nx) Surface topography \\(m\\) smb (ny,nx) Surface Mass Balance \\(m~y^{-1}\\) ice-eq ubar (ny,nx) x- depth-average velocity of ice \\(m~y^{-1}\\) vbar (ny,nx) y- depth-average velocity of ice \\(m~y^{-1}\\) U (nz,ny,nx) x-horiz. 3D velocity field of ice \\(m~y^{-1}\\) V (nz,ny,nx) y-horiz. 3D velocity field of ice \\(m~y^{-1}\\) W (nz,ny,nx) z-vert.  3D velocity field of ice \\(m~y^{-1}\\) arrhenius (ny,nx) Arrhenius Factor \\(MPa^{-3}~y^{-1}\\) slidingco (ny,nx) Sliding Coefficient \\(MPa~m^{-1/3}~y^{-1/3}\\) divflux (ny,nx) Divergence of the flux \\(m~y^{-1}\\) icemask (ny,nx) Mask to restrict the smb comp. - dtopgdt (ny,nx) Erosion rate \\(m~y^{-1}\\) xpos,ypos (nb particles) x,y position of particles \\(m\\) rhpos (nb particles) rel. pos of particles in ice column \\(m\\) air_temp (nt,ny,nx) seasonal air temperature 2 m above ground \u00b0C precipitation (nt,ny,nx) seasonal precipitation (water eq) \\(kg~m^{-2}~y^{-1}\\)"},{"location":"examples/Examples/","title":"Examples","text":"<p>Once IGM is installed, it is time to make your first runs. The best and quickest way to get to know IGM is to run the given IGM examples. For that, you may download dedicated IGM examples or run the <code>quick-demo</code> presented hereafter if you want to go faster.</p>"},{"location":"examples/Examples/#igm-examples","title":"IGM examples","text":"<p>Download the following repository that contains a gallery of ready-to-run setups (incl. parameter file, data, and user modules if any):</p> <pre><code>git clone https://github.com/instructed-glacier-model/igm-examples\n</code></pre> <p>Then, you may simply run the command <code>igm_run +experiment=params</code> (or replace <code>params</code> with the actual name of your parameter file) in the corresponding folder to execute the example.</p>"},{"location":"examples/Examples/#quick-demo","title":"Quick demo","text":"<p>Copy and paste the following YAML parameter file (name it <code>params.yaml</code>), place it in a folder named <code>experiment</code>, and then run the command <code>igm_run +experiment=params</code>. This will model the Glacier with RGI ID <code>RGI60-11.01450</code> (the Great Aletsch Glacier, Switzerland) from 2020 to 2100, assuming a temperature increase of 4 degrees by 2100 relative to 1960\u20131990. </p> <p>The run will: 1. Use the <code>oggm_shop</code> module to download all necessary data via OGGM. 2. Execute a forward model combining the OGGM-based SMB model and the IGM-based ice flow model. 3. Write and plot the results in real time.</p> <p>Warning</p> <p>This setup is provided as an example and has not been validated against historical data. It should not be interpreted as a scientifically accurate simulation. After running this example, you can explore different glaciers by selecting a different ID (refer to the GLIMS Viewer), modify parameters, or experiment with additional modules.</p> params.yaml<pre><code># @package _global_\n\ndefaults:\n  - override /inputs: \n    - oggm_shop\n    - local\n  - override /processes: \n    - clim_oggm\n    - smb_oggm\n    - iceflow\n    - time\n    - thk\n  - override /outputs: \n    - write_ncdf\n    - plot2d\n\ninputs:\n  oggm_shop:\n    RGI_ID: \"RGI60-11.01450\"\n    RGI_version: 6\n\nprocesses:\n  clim_oggm:\n    clim_trend_array: \n      - [\"time\", \"delta_temp\", \"prec_scal\"]\n      - [ 2020,           0.0,         1.0]\n      - [ 2100,           4.0,         1.0]\n  iceflow:\n    physics:\n      init_slidingco: 0.25\n  time:\n    start: 2200.0\n    end: 2100.0\n    save: 10.0\n\noutputs:\n  plot2d:\n    live: true\n</code></pre>"},{"location":"hydra/basics/","title":"Configuring IGM","text":"<p>In order to use Hydra with IGM, one needs to understand how the general structure works. Each IGM run requires an experiment that outlines all modules, constants, and instructions. Before IGM starts running its code, Hydra will then combine all the parameters specified in the experiment file, and command line, into a single structure. We will explore in the following sections how to adjust these parameters using Hydra's workflow.</p>"},{"location":"hydra/basics/#configuring-igm-through-the-experiment-file","title":"Configuring IGM through the Experiment File","text":"<p>For the first example, lets assume that we want to specify all our parameters in the <code>experiment</code> file. To demonstrate this, lets run the aletsch-basic example with the following command</p> <pre><code>igm_run +experiment=params\n</code></pre> <p>This command might produce a configuration structure like so</p> <pre><code>core:\n    ...\nprocesses:\n  smb_simple:\n    ...\n  iceflow:\n    ...\n  time:\n    ...\n  thk:\n    ...\n  vert_flow:\n    ...\n  particles:\n    ...\ninputs:\n  local:\n    ...\noutputs:\n  local:\n    ...\n  plot2d:\n    ...\n</code></pre> <p>This shows all the input, process, and output modules along with all their parameters. This structure comes directly from the <code>params</code> file that has the following structure</p> <pre><code>core:\n    ...\n\ndefaults:\n  - override /inputs: \n    - local\n  - override /processes: \n    - smb_simple\n    - iceflow\n    - time\n    - thk\n    - vert_flow\n    - particles\n  - override /outputs: \n    - local\n    - plot2d\n\nprocesses:\n  smb_simple:\n    ...\n  time:\n    ...\n\ninputs:\n  local:\n    ...\n\noutputs:\n    ...\n</code></pre> <p>Now, lets break down this structure. At the outermost indentation level (that is the level that <code>core</code>, <code>processes</code>, <code>inputs</code>, and <code>outputs</code> lie on), we say we are at the <code>global</code> level. This structure was dictated by IGM's structure, and thus, every experiment file must follow the same structure if one wants to access and override parameters correctly.</p>"},{"location":"hydra/basics/#using-igms-default-modules","title":"Using IGM's Default Modules","text":"<p>If we would just want to use IGM's default modules and default parameters (which would rarely be the case), we could simply specify all modules in the <code>defaults</code> list inside our <code>params</code> file.</p> <p>Align with the Global Level</p> <p>Do not forget to include <code>@package _global_</code> inside your params file, or else IGM will structure it incorrectly. If you do forget it, it will not overwrite the default parameters and will just create a separate section with the parameters you specified. It will also not necessarily throw an error, so please verify that your parameters are correctly overwriting the default ones.</p> <p><pre><code>@package _global_\n\ndefaults:\n  - override /inputs: \n    - local\n  - override /processes: \n    - smb_simple\n    - iceflow\n    - time\n    - thk\n    - vert_flow\n    - particles\n  - override /outputs: \n    - local\n    - plot2d\n</code></pre> However, most of the time we would like to adjust the default parameters. In order to do this, we can specify the parameters in an additional section in the same level as <code>defaults</code>. For instance, if we wanted to change the default time frame our simulation runs, we could make the following adjustment</p> <pre><code>@package _global_\n\ndefaults:\n  - override /inputs: \n    - local\n  - override /processes: \n    - smb_simple\n    - iceflow\n    - time\n    - thk\n    - vert_flow\n    - particles\n  - override /outputs: \n    - local\n    - plot2d\n\nprocesses:\n  time:\n    start: 1900.0\n    end: 2000.0\n    save: 10.0\n</code></pre> <p>In order to check to see if our parameters have successfully overwritten the default ones, we can see the final configuration structure with the following command</p> <pre><code>igm_run +experiment=params --help\n</code></pre> <p>or alternatively,</p> <pre><code>igm_run +experiment=params --cfg job\n</code></pre> <p>If you notice any interpolated values, that is values in the configuration that are variables (like <code>cwd: ${get_cwd:0}</code>), you can resolve these and get the value by using</p> <pre><code>igm_run +experiment=params --cfg job --resolve\n</code></pre> <p>Note that this is the final configuration structure for the <code>params</code> experiment. If you want to show the configuration IGM has by default or for another experiment, please change it accordingly. </p>"},{"location":"hydra/basics/#configuring-igm-through-the-command-line","title":"Configuring IGM through the Command Line","text":"<p>Simularly, if one wants to keep the <code>params</code> file the same but specify changes to specifc parameters, we can simply modify them within the command line. For instance, lets say we want to use the <code>params</code> file but just change the start date. We can do so by running</p> <pre><code>igm_run +experiment=params processes.time.start=1990\n</code></pre> <p>which will result in</p> <pre><code>@package _global_\n\ndefaults:\n  - override /inputs: \n    - local\n  - override /processes: \n    - smb_simple\n    - iceflow\n    - time\n    - thk\n    - vert_flow\n    - particles\n  - override /outputs: \n    - local\n    - plot2d\n\nprocesses:\n  time:\n    start: 1990.0\n    end: 2000.0\n    save: 10.0\n</code></pre> <p>We can do this for multiple parameters at the same time. For instance, the start and the end date: <pre><code>igm_run +experiment=params processes.time.start=1990 process.time.end=2050\n</code></pre></p>"},{"location":"hydra/custom_configurations/","title":"Configuring Custom Modules","text":"<p>In some of the IGM examples, you may notice that the default modules IGM provides are not enough. In this case, one can create a custom module and add it to the configuration structure. For instance, lets now explore the aletsch 1880-2100 example.</p> <p>In the <code>experiment</code> file, you will now notice a slightly different structure. Everything stays the same except now we have our custom modules specified in the <code>defaults</code> section.</p> <pre><code># @package _global_\n\ncore:\n    ...\n\ndefaults:\n\n  - /user/conf/processes@processes.smb_accmelt: smb_accmelt \n  - /user/conf/processes@processes.clim_aletsch: clim_aletsch\n  - /user/conf/processes@processes.track_usurf_obs: track_usurf_obs\n\n  - override /inputs: \n     - local\n  - override /processes: \n     - track_usurf_obs\n     - clim_aletsch\n     - smb_accmelt\n     - iceflow\n     - time\n     - thk\n     - rockflow\n     - vert_flow\n     - particles\n  - override /outputs: \n     - local\n     - plot2d\n\ninputs:\n    ...\nprocesses:\n    ...\noutputs:\n    ...\n</code></pre> <p>This will import our custom modules configuration files (not the code) so that it will now be part of the final configuration structure. Lets now break down what the following line means</p> <pre><code>- /user/conf/processes@processes.smb_accmelt: smb_accmelt \n</code></pre> <p>In essence, this line means the following</p> <pre><code>- [FILE LOCATION]@[POSITION IN STRUCTURE]: [NAME OF FILE]\n</code></pre> <p>For example, from the custom modules page, we know that every custom module should follow this structure</p> <pre><code>.\n\u2514\u2500\u2500 user\n  \u251c\u2500\u2500 code\n  \u2502   \u2514\u2500\u2500 input\n  \u2502   \u2502   \u2514\u2500\u2500 my_module.py\n  \u2502   \u2514\u2500\u2500 processes\n  \u2502   \u2502   \u2514\u2500\u2500 my_module.py\n  \u2502   \u2514\u2500\u2500 outputs\n  \u2502       \u2514\u2500\u2500 my_module.py\n  \u2514\u2500\u2500 conf\n    \u2514\u2500\u2500 input\n    \u2502   \u2514\u2500\u2500 my_module.yaml\n    \u2514\u2500\u2500 processes\n    \u2502   \u2514\u2500\u2500 my_module.yaml\n    \u2514\u2500\u2500 outputs\n      \u2514\u2500\u2500 my_module.yaml\n</code></pre> <p>Here, our configuration for our custom process, <code>smb_accmelt</code> is located in <code>/user/conf/processes</code>.</p> <pre><code>.\n\u2514\u2500\u2500 user\n  \u2514\u2500\u2500 conf\n    \u2514\u2500\u2500 processes\n       \u2514\u2500\u2500 smb_accmelt.yaml\n</code></pre> <p>If we were to change this line into</p> <pre><code>- /user/conf/processes@processes.smb_accmelt: smb_accmelt_other_name \n</code></pre> <p>Hydra would not be able to find the file and will say</p> <p><pre><code>In 'experiment/params': Could not find 'user/conf/processes/smb_accmelt_other_name'\n\nAvailable options in 'user/conf/processes':\n    clim_aletsch\n    smb_accmelt\n    track_usurf_obs\n</code></pre> Additionally, we can change the position in the configuration file using the <code>@</code> operator. In Hydra, these are called packages (read more here). Recall that in the <code>params</code> file, one must include the <code># @package _global_</code> header. For custom configurations in the <code>/user/conf/</code> folder, there is a similar structure. For instance, in the <code>smb_accmelt</code>, by default it operates on the <code>_global_</code> level. This means that in the <code>experiment</code> file, when we specify <code>@processes.smb_accmelt</code> it assumes this is relative to the <code>_global_</code> level, which is what we want. The following two cases are equivalent:</p>"},{"location":"hydra/custom_configurations/#case-1","title":"Case 1","text":"<p>In this case, we specify the position in the <code>params.yaml</code> and not in the <code>smb_accmelt.yaml</code> header. We do this with the <code>@processes.smb_accmelt</code> extension.</p> params.yaml<pre><code>- /user/conf/processes@processes.smb_accmelt: smb_accmelt\n</code></pre> smb_accmelt.yaml<pre><code>update_freq: 1\nweight_ablation: 1.25\nweight_accumulation: 1.0\nthr_temp_snow: 0.5\nthr_temp_rain: 2.5\nshift_hydro_year: 0.75\nice_density: 910.0\nwat_density: 1000.0\nweight_Aletschfirn: 1.0\nweight_Jungfraufirn: 1.0\nweight_Ewigschneefeld: 1.0\n</code></pre>"},{"location":"hydra/custom_configurations/#case-2","title":"Case 2","text":"<p>Alternatively, in the <code>smb_accmelt.yaml</code> we can change this level with the <code>@package</code> header instead of specifying it within the <code>params</code> file. For example, in the <code>smb_accmelt.yaml</code> file, we can add <code>@package processes.smb_accmelt</code> and then in our <code>params</code> file, we can simply just import the file without using the <code>@</code> operator:</p> params.yaml<pre><code>- /user/conf/processes: smb_accmelt\n</code></pre> smb_accmelt.yaml<pre><code># @package processes.smb_accmelt\n\nupdate_freq: 1\nweight_ablation: 1.25\nweight_accumulation: 1.0\nthr_temp_snow: 0.5\nthr_temp_rain: 2.5\nshift_hydro_year: 0.75\nice_density: 910.0\nwat_density: 1000.0\nweight_Aletschfirn: 1.0\nweight_Jungfraufirn: 1.0\nweight_Ewigschneefeld: 1.0\n</code></pre> <p>In general, Hydra allows the user to have a modular and complex configuration of files that ultimately get combined into a final configuration structure. This structure is then read by IGM to initialize the simluation run. Apart from the obvious benefits from managing complex structures, Hydra also allows for easy reproducability as the configurations are tracked every single run as well as distributed computing as it can launch ensemble runs and integrate into slurm and other computing platforms (Ray). To learn more, please continue onto the next sections.</p>"},{"location":"hydra/distributed_computing/","title":"Distributed Computing","text":"<p>Another advantage with Hydra is that it is well suited for distributed computing. Specifically, it is not just meant for distributed computing but running experiments at scale. Previously, we learned that each IGM run requires a single experiment file (e.g. <code>params.yaml</code>). However, instead of running just one experiment, we can run multiple. We can do this with Hydra's flag <code>--multirun</code>.</p>"},{"location":"hydra/distributed_computing/#using-multirun","title":"Using Multirun","text":"<p>For example, lets say that I want to run the same experiment but just with two different values for one of my modules. In this example, lets take the <code>aletsch-basic</code> example's setup</p> param.yaml<pre><code># @package _global_\n\ncore:\n  url_data: https://www.dropbox.com/scl/fo/kd7dix5j1tm75nj941pvi/h?rlkey=q7jtmf9yn3a970cqygdwne25j&amp;dl=0\n  logging: True\n  logging_level: 30\n\ndefaults:\n  - override /inputs: \n    - local\n  - override /processes: \n    - smb_simple\n    - iceflow\n    - time\n    - thk\n    - vert_flow\n    - particles\n  - override /outputs: \n    - local\n    - plot2d\n\ninputs:\n  local:\n     filename: input.nc\n     coarsening:\n       ratio: 1\n\nprocesses:\n  smb_simple:\n    array:\n      - [\"time\", \"gradabl\", \"gradacc\", \"ela\", \"accmax\"]\n      - [1900, 0.009, 0.005, 2800, 2.0]\n      - [2000, 0.009, 0.005, 2900, 2.0]\n      - [2100, 0.009, 0.005, 3300, 2.0]\n  time:\n    start: 1900.0\n    end: 2000.0\n    save: 10.0\n\noutputs:\n  plot2d:\n    live: False\n</code></pre> <p>Now, lets assume I want to run two experiments: one with <code>start=1900</code> and another with <code>start=1950</code>. Instead of manually running two experiments back-to-back, I can run the following command</p> <p>Warning</p> <p>Make sure you add <code>--multirun</code> at the end of the command. Additionally, watch out for spaces (i.e. <code>1990,1950</code> and not <code>1990, 1950</code>).</p> <pre><code>igm_run +experiment=params processes.time.start=1990,1950 --multirun\n</code></pre> <p>which will result in Hydra launching two jobs sequentially. Importantly, the default bahvior is to use the <code>basic</code> launcher, which means that the jobs will be run one at a time. We can verify this by looking at the terminal, where we see the message</p> <pre><code>[2025-04-08 11:34:59,755][HYDRA] Launching 2 jobs locally\n[2025-04-08 11:34:59,755][HYDRA]    #0 : +experiment=params processes.time.start=1990\n...\nCODE IS RUNNING HERE\n...\n</code></pre> <p>However, most jobs are independent, so instead we should ideally launch both jobs at the same time. This is where we can use the <code>joblib</code> launcher instead. To do this, we can instead use the command</p> <p>Note</p> <p>Make sure to install it beforehand with <code>pip install hydra-joblib-launcher --upgrade</code></p> <pre><code>igm_run +experiment=params processes.time.start=1990,1950 hydra/launcher=joblib --multirun\n</code></pre> <p>which will produce the following output</p> <pre><code>[2025-04-08 11:43:51,156][HYDRA] Joblib.Parallel(n_jobs=-1,backend=loky,prefer=processes,require=None,verbose=0,timeout=None,pre_dispatch=2*n_jobs,batch_size=auto,temp_folder=None,max_nbytes=None,mmap_mode=r) is launching 2 jobs\n[2025-04-08 11:43:51,156][HYDRA] Launching jobs, sweep output dir : multirun/2025-04-08/11-43-50\n[2025-04-08 11:43:51,156][HYDRA]    #0 : +experiment=params processes.time.start=1990\n[2025-04-08 11:43:51,156][HYDRA]    #1 : +experiment=params processes.time.start=1950\n</code></pre> <p>Here we see that both jobs were launched in parallel instead of one at at a time. We can also see that there are various parameters we can choose like the number of jobs or the batch size. For an exhaustive list, please go to Hydra's page.</p>"},{"location":"hydra/distributed_computing/#grid-searches","title":"Grid Searches","text":"<p>One cool extension of this is that it is very easy to launch a multitude of jobs where Hydra will simply launch all possible combinations of the parameters. For instance, lets say that I do not only want to change the start date but also the end date and try all 4 combinations. To do this, I use the same command but just specify the end date and include a comma between potential values:</p> <pre><code>igm_run +experiment=params processes.time.start=1990,1950 processes.time.end=2000,2050 hydra/launcher=joblib --multirun\n</code></pre> <p>which will produce the following message from Hydra</p> <pre><code>[2025-04-08 12:04:41,425][HYDRA] Joblib.Parallel(n_jobs=-1,backend=loky,prefer=processes,require=None,verbose=0,timeout=None,pre_dispatch=2*n_jobs,batch_size=auto,temp_folder=None,max_nbytes=None,mmap_mode=r) is launching 4 jobs\n[2025-04-08 12:04:41,425][HYDRA] Launching jobs, sweep output dir : multirun/2025-04-08/12-04-40\n[2025-04-08 12:04:41,425][HYDRA]    #0 : +experiment=params processes.time.start=1990 processes.time.end=2000\n[2025-04-08 12:04:41,425][HYDRA]    #1 : +experiment=params processes.time.start=1990 processes.time.end=2050\n[2025-04-08 12:04:41,425][HYDRA]    #2 : +experiment=params processes.time.start=1950 processes.time.end=2000\n[2025-04-08 12:04:41,425][HYDRA]    #3 : +experiment=params processes.time.start=1950 processes.time.end=2050\n</code></pre> <p>In addition to grid searchs, Hydra actually also allows for smarter searches using Beysian approaches or other inverse algorithms. To learn more, please visit their sweepers section.</p>"},{"location":"hydra/distributed_computing/#running-on-node-clusters","title":"Running on Node Clusters","text":"<p>Here we will explore a basic example of when you have multiple GPU devices and want to run IGM on both devices for respective experiments. Above, you learned how to launch 2 experiments at the same time. If we only have a single GPU device, IGM will try to use the same GPU for both experiments, which will result in potential conflicts. To combat this, we can still use <code>joblib</code> but when on a cluster of multple GPUs. For example, lets say our node looks like the following (when running <code>nvidia-smi</code>)</p> <pre><code>Tue Apr  8 11:52:52 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 4090        On  | 00000000:02:00.0 Off |                  Off |\n| 90%   27C    P8              16W / 450W |      0MiB / 24564MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  NVIDIA GeForce RTX 4090        On  | 00000000:83:00.0 Off |                  Off |\n| 90%   26C    P8              18W / 450W |      0MiB / 24564MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</code></pre> <p>Here you will see two GPUs: <code>NVIDIA RTX 4090</code>. Now, lets still use the <code>joblib</code> launcher, but before doing so, specify a different GPU device for each run. To do so, we can locate the GPU ID on the far left (in this case, we have <code>0</code> and <code>1</code>) and either in our <code>params</code> file or the commandline, we can select the GPU device. In our case, lets say we have two experiments</p> <p>experiment_1<pre><code>core:\n  hardware:\n    visible_gpus: [0]\n...\n</code></pre> experiment_2<pre><code>core:\n  hardware:\n    visible_gpus: [1]\n...\n</code></pre></p> <p>Then, by simply doing</p> <pre><code>igm_run +experiment=experiment_1,experiment_2 hydra/launcher=joblib --multirun\n</code></pre> <p>we launch both experiments on both GPUs.</p> <p>Hydra also allows one to use SLURM scipts as well as AWS clusters (using Ray). While we do not currently have an example, we recommend you to again go to Hydra's page to learn more.</p>"},{"location":"hydra/help/","title":"Help","text":""},{"location":"hydra/help/#application-help","title":"Application Help","text":"<p>As explained above, we can use <code>igm_run +experiment=params --help</code> to get the final configuration structure that Hydra reads. However, it can also offer insight into the configurations one can potentially use with IGM. For instance, running the above command will yield a message that looks like so (for the aletsch 1880-2100 example):</p> <pre><code>== Configuration groups ==\nCompose your configuration from those groups (group=option)\n\ninputs: load_ncdf, load_tif, local, oggm_shop\noutputs: local, plot2d, write_ncdf, write_tif, write_ts\nprocesses: avalanche, clim_glacialindex, clim_oggm, enthalpy, gflex, glerosion, iceflow, particles, read_output, rockflow, smb_accpdd, smb_oggm, smb_simple, texture, thk, time, vert_flow\n\n\n== Config ==\nOverride anything in the config (foo.bar=value)\n\n...\n</code></pre> <p>At the moment, the above modules listed are only the built in modules from IGM and do not include any custom modules. However, this may change in a future release if it is found out to be possible.</p> <p>We can also use the help specific to Hydra in case we are unsure how to use it with <code>igm_run +experiment=params --hydra-help</code> which may produce something like so</p> <pre><code>== Flags ==\n--help,-h : Application's help\n--hydra-help : Hydra's help\n--version : Show Hydra's version and exit\n--cfg,-c : Show config instead of running [job|hydra|all]\n--resolve : Used in conjunction with --cfg, resolve config interpolations before printing.\n--package,-p : Config package to show\n--run,-r : Run a job\n--multirun,-m : Run multiple jobs with the configured launcher and sweeper\n--shell-completion,-sc : Install or Uninstall shell completion:\n    Bash - Install:\n    eval \"$(igm_run -sc install=bash)\"\n    Bash - Uninstall:\n    eval \"$(igm_run -sc uninstall=bash)\"\n\n    Fish - Install:\n    igm_run -sc install=fish | source\n    Fish - Uninstall:\n    igm_run -sc uninstall=fish | source\n\n    Zsh - Install:\n    Zsh is compatible with the Bash shell completion, see the [documentation](https://hydra.cc/docs/1.2/tutorials/basic/running_your_app/tab_completion#zsh-instructions) for details.\n    eval \"$(igm_run -sc install=bash)\"\n    Zsh - Uninstall:\n    eval \"$(igm_run -sc uninstall=bash)\"\n\n--config-path,-cp : Overrides the config_path specified in hydra.main().\n                    The config_path is absolute or relative to the Python file declaring @hydra.main()\n--config-name,-cn : Overrides the config_name specified in hydra.main()\n--config-dir,-cd : Adds an additional config dir to the config search path\n--experimental-rerun : Rerun a job from a previous config pickle\n--info,-i : Print Hydra information [all|config|defaults|defaults-tree|plugins|searchpath]\nOverrides : Any key=value arguments to override config values (use dots for.nested=overrides)\n\n== Configuration groups ==\nCompose your configuration from those groups (For example, append hydra/job_logging=disabled to command line)\n\nhydra: config\nhydra/env: default\nhydra/help: default\nhydra/hydra_help: default\nhydra/hydra_logging: default, disabled, hydra_debug, none\nhydra/job_logging: default, disabled, none, stdout\nhydra/launcher: basic, joblib\nhydra/output: default\nhydra/sweeper: basic\n</code></pre> <p>Of course, we could go more in depth, but for the moment, this information should suffice. For eager people, I suggest you learn more on the Hydra website.</p>"},{"location":"hydra/introduction/","title":"Hydra","text":"<p>As IGM runs various modules based on physical processes (for instance iceflow), one needs to also be able to adjust physical parameters and constants between runs. For instance, if one wants to change the ice viscosity, it would be ideal to adjust this parameter outside the source code in a controlled manner. Now, imagine where one can not only adjust the viscosity but hundreds of different parameters across a multitude of modules - this is where Hydra comes in. Simply put, it is a way to manage a complex hierarchy of configuration files. In more detail, we chose to integrate Hydra into IGM for the following reasons:</p> <ul> <li> <p>Readability and reproducability: As Hydra uses the yaml extension, files are by consequence much easier to read. Hydra also excels in keeping track of which parameters you change, so for scientific experiments, it is much easier for results to become reproducable.</p> </li> <li> <p>Scalability: While it is certainly possible to have IGM work on multiple computers, GPUs, etc., there is a lot of manual work involved in setting this up and maintaining it. Fortunately, Hydra automatically takes care of many of these requirements. For instance, we can launch ensemble runs quite easily with a single line, optimize our parameters for inverse problems, and integrate our system into slurm directly. For more information, please visit this page.</p> </li> <li> <p>Abstraction: Lastly, but arguably most importantly, Hydra excels in managing complex hierarchies of yaml files. If we were to manage this ourselves, it quickly becomes error prone and acts as a barrier to how organized IGM can become. We chose Hydra to let it manage our structure in hopes that it will robustify our existing codebase whilst future proofing it.</p> </li> </ul> <p>To start learning how to use Hydra with IGM, please visit the configuration and distributed computing pages. These pages serve as a quick reference, so if you require more information, we suggest you go to the Hydra website itself for more examples and explanations.</p>"},{"location":"hydra/introduction_old/","title":"Hydra","text":"<p>In order to launch IGM with different parameters, each run is typically accompanied with a corresponding <code>params</code> file. This file is responsible for the selection of modules (i.e. an input, process, or output module), specifying variables and constants within the modules, and many other tasks. While there are many packages that serve this purpose, we chose to use the hydra package (previously, it was <code>argparse</code>). It has been extensively used for machine learning experiments and tuning complex parameters. For this reason, all <code>param</code> files now use the yaml extension, aiding in further readability and organization. In short, our reasons to specifically use this package are the following</p> <p>Readability and reproducability: As Hydra uses the yaml extension, files are by consequence much easier to read. Hydra also excels in keeping track of which parameters you change, so for scientific experiments, it is much easier for results to become reproducable.</p> <p>Scalability: While it is certainly possible to have IGM work on multiple computers, GPUs, etc., there is a lot of manual work involved in setting this up and maintaining it. Fortunately, Hydra automatically takes care of many of these requirements. For instance, we can launch ensemble runs quite easily with a single line, optimize our parameters for inverse problems, and integrate our system into slurm directly. For more information, please visit()[].</p> <p>Abstraction: Lastly, but arguably most importantly, hydra excels in managing complex hierarchies of yaml files. If we were to manage this ourselves, it quickly becomes error prone and acts as a barrier to how organized IGM can become. We chose Hydra to let it manage our structure in hopes that it will robustify our existing codebase whilst future proofing it.</p>"},{"location":"hydra/introduction_old/#how-hydra-works","title":"How Hydra Works","text":"<p>In this section, we will explore a bit on how Hydra works with IGM. For full details, we suggest you go here and look at some of the tutorials.</p> <p>Each IGM run, like before, takes a single <code>params</code> file to choose modules and initialize constants. In this file, we can specify which modules we want and their following parameters. As seen in the </p> <pre><code># @package _global_\n\ncore:\n    ...\n\ndefaults:\n  - override /inputs: \n    ...\n  - override /processes: \n    ...\n  - override /outputs: \n    ...\n\ninputs:\n    ...\n\nprocesses:\n    ...\n\noutputs:\n    ...\n</code></pre>"},{"location":"hydra/working_directory/","title":"Working Directory","text":""},{"location":"hydra/working_directory/#working-directory","title":"Working Directory","text":"<p>By default, IGM will use the current working directory wherever you use the <code>igm_run</code> command (and thus save all the results to this directory). While it could be possible to change the working directory to run IGM outside of where the <code>experiment</code> folder is located, we suggest you use IGM as intended as it could break some other modules. However, we can easily change the output directories so that our results are stored for each run and not overwritten. To do this, we can specify to create a new output directory for each and every run by adding the following lines </p> params.yaml<pre><code>hydra:\n  job:\n    chdir: True\n...\n</code></pre> <p>Consequently, each IGM run will then generate a folder with the day as the parent folder and the time as a subfolder. For instance, after a few runs we could end up with the following structure</p> <p><pre><code>\u2514\u2500\u2500\u25002025-04-07\n    \u251c\u2500\u2500\u2500 12-06-29\n    \u2502     \u2514\u2500\u2500\u2500 .hydra\n    \u2502         \u251c\u2500\u2500\u2500 config.yaml\n    \u2502         \u251c\u2500\u2500\u2500 hydra.yaml\n    \u2502         \u2514\u2500\u2500\u2500 overrides.yaml\n    \u2514\u2500\u2500\u2500 12-07-38\n          \u2514\u2500\u2500\u2500 .hydra\n              \u251c\u2500\u2500\u2500 config.yaml\n              \u251c\u2500\u2500\u2500 hydra.yaml\n              \u2514\u2500\u2500\u2500 overrides.yaml\n</code></pre> One can customize these folder names if they want instead of just having the day and time as the folder names. To do so, please read more here.</p>"},{"location":"installation/linux/","title":"Linux","text":"<ol> <li>Install NVIDIA drivers</li> </ol> <p>If you aim to use only the CPU or already get an output from <code>nvidia-smi</code>, you can skip this step.</p> <p><pre><code># get the latest libraries from apt\nsudo apt update\nsudo apt upgrade\n\n# choose which driver version is compatible with your GPU device (in this case 510)\nsudo apt install nvidia-driver-510 nvidia-dkms-510\nsudo reboot # you wont see the changes until after you reboot\n</code></pre> After rebooting, you can check your driver version with the command <code>watch -d -n 0.5 nvidia-smi</code> should give you live information on your GPU device.</p> <ol> <li>Install anaconda and create a virtual environment (strongly recommended) with conda or venv:</li> </ol> <pre><code># install anaconda\nwget https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh\nbash Anaconda3-2023.09-0-Linux-x86_64.sh\n\n# create new environment\nconda create --name igm python=3.10\n\n# activate environment to install IGM\nconda activate igm\n</code></pre> <p>or</p> <pre><code># create igm venv environment\npython3.10 -m venv igm\n\n# activate environment to install IGM\nsource igm/bin/activate\n</code></pre> <ol> <li>Install IGM</li> </ol> <p>For simple usage, you can install the latest IGM stable version and its dependencies from the Pypi as follows:</p> <pre><code>pip install igm_model\n</code></pre> <p>OR for using all and recent features, you can install the IGM development version from the github repository as follows:</p> <pre><code>git clone https://github.com/jouvetg/igm.git\ncd igm\npip install -e .\n</code></pre> <p>After that, you may run any example (<code>igm_run</code>). As IGM is being updated often, make sure you have the latest version, you may run</p> <pre><code>git pull\n</code></pre>"},{"location":"installation/mac/","title":"Mac","text":"<p>IGM core library native Tensorflow is not supported on Mac for GPU usage. Instead, a \"Tensorflow for Mac\", called tensorflow-metal, was developed as workaround. To install IGM on Mac, you may follow the linux workflow, however, you will need to change in setup.py tensorflow by tensorflow-macos. Here is a working procedure (tested on MacBook Pro M2) -- still we recommend using a virtual environment such as conda or venv as on linux:</p> <p><pre><code>git clone -b develop https://github.com/jouvetg/igm\ncd igm\n</code></pre> You need to edit \"install_requires=[...]\" in the file \"setup.py\":</p> <ul> <li>To use only the CPUs: <code>tensorflow-macos==2.14.0</code></li> <li>To use the GPUs: <code>tensorflow-macos==2.14.0, tensorflow-metal,</code></li> </ul> <p>and then <pre><code>pip install -e .\n</code></pre></p>"},{"location":"installation/quick_start/","title":"Quick Start","text":"<p>This guide serves as the fastest way to install IGM. It assumes that have already</p> <ol> <li>Downloaded the nvidia drivers</li> <li>Have a working virtual environment</li> </ol> <p>If this is the case, you can skip to the next section.</p> <p>Installing on Windows</p> <p>Tensorflow does not allow us to run IGM on GPU directly on Windows, and the module <code>oggm_shop</code> does not work on windows. Therefore, we recommend windows-user to install WSL2-ubuntu, which provides a linux/ubuntu terminal. WSL2 terminal can be nicely linked with VS code (with an extension). First, install WSL2-ubuntu</p> <pre><code>wsl --install Ubuntu-22.04\nsudo apt update\nsudo apt upgrade\n</code></pre>"},{"location":"installation/quick_start/#installing-methods","title":"Installing Methods","text":"<p>Once this is done, the options are the following</p> <ol> <li>pip</li> <li>github</li> </ol>"},{"location":"installation/quick_start/#pip","title":"Pip","text":"<p>To install the latest version of IGM, simply run</p> <pre><code>pip install igm_model\n</code></pre> <p>For reproducibility purposes, one might want to install a specific version of IGM. In order to do this, one can specify the version (note, this version must exist on the PyPI servers).</p> <pre><code>pip install igm_model=='2.2.2'\n</code></pre>"},{"location":"installation/quick_start/#github","title":"Github","text":"<p>If one wants to have the latest versions, or even, work on a specific hash for reproducibility, one can download IGM through the github repository. This is useful for developers, and researchers alike, who want to have the latest features as well as contribute to IGM's model personally.</p> <p>One can download the latest version of IGM with <code>git clone</code></p> <pre><code>git clone https://github.com/jouvetg/igm.git\n</code></pre> <p>A Note about IGM's Install Location</p> <p>Please note that where you decide to clone IGM is purely the location where IGMs source code will be installed. After you install IGM with <code>pip install -e</code> (more on this below), you can run IGM from any location on your computer. This is because installing IGM will create a symbolic link to wherever this folder is installed.</p> <p>Installing on Mac</p> <p>IGM's core package, Tensorflow, is unfortunately not natively supported on Mac OS for GPUs. Instead, a \"Tensorflow for Mac\", called tensorflow-metal, was developed as a workaround. To install IGM on Mac, you can still clone the repository with the above line, but you must additionally change <code>tensorflow</code> to <code>tensorflow-macos</code> in <code>setup.py</code> before running <code>pip install -e</code>. Here is a working procedure (tested on MacBook Pro M2); we still recommend using a virtual environment such as conda or venv when installing.</p> <p><pre><code>git clone https://github.com/jouvetg/igm\ncd igm\n</code></pre> Now, in the <code>setup.py</code> file, you will need to edit the \"install_requires=[...]\" line depending on your requirements:</p> <ul> <li>To use only the CPU: <code>tensorflow-macos==2.14.0</code></li> <li>To use the GPU: <code>tensorflow-macos==2.14.0, tensorflow-metal,</code></li> </ul> <p>Now, once the <code>setup.py</code> file is ready for your machine and operating system, one can install IGM inside his or her virtual environment. To do this, run the following command in the same level as the <code>setup.py</code> file:</p> <pre><code>pip install -e .\n</code></pre> <p>Note that while the user installs </p> <p>As IGM is being updated often, make sure you have the latest version by running the following command inside the <code>igm</code> folder</p> <pre><code>git pull\n</code></pre>"},{"location":"installation/windows/","title":"Windows","text":"<p>Tensorflow does not allow us to run IGM on GPU directly on Windows, and the module <code>oggm_shop</code> does not work on windows. Therefore, we recommend windows-user to install WSL2-ubuntu, which provides a linux/ubuntu terminal. WSL2 terminal can be nicely linked with VS code (with an extension). First, install WSL2-ubuntu</p> <pre><code>wsl --install Ubuntu-22.04\nsudo apt update\nsudo apt upgrade\n</code></pre> <p>and then, install the NVIDIA drivers if not done (if you get no output from <code>nvidia-smi</code>), and if you wish to use the GPU.</p> <p>The rest -- installation of conda or venv environment and the installation of IGM -- are the same as above on Linux.</p>"},{"location":"installation/other/nvidia_drivers/","title":"Installing Nvidia Drivers","text":"<p>Assuming you have a linux system or installed wsl windows, we can now install the necessary Nvidia Drivers so that tensorflow can find them automatically. It is important to note that from tensorflow 2.14, tensorflow only needs the Nvidia Drivers and nothing else. Previously, one would have to install the drivers, cuda, cudnn, etc.</p>"},{"location":"installation/other/nvidia_drivers/#do-i-already-have-an-nvidia-driver","title":"Do I already have an nvidia driver?","text":"<p>To determine if you already have an nvidia-driver, you can simply run the command</p> <pre><code>nvidia-smi\n</code></pre> <p>which should produce an output like this (if you have an nvidia-driver)</p> <pre><code>+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA RTX A3000 12GB La...    Off | 00000000:01:00.0 Off |                  Off |\n| N/A   50C    P0              N/A /  90W |      8MiB / 12288MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      3914      G   /usr/lib/xorg/Xorg                            4MiB |\n+---------------------------------------------------------------------------------------+\n</code></pre> <p>Above, you will see your driver version. It is important to note that even though it says the CUDA version is 12.2, this is the maximum supported version of CUDA given the driver version, not the actual version of your installed CUDA. Now that your machine has a Nvidia driver, you can move onto installing IGM with <code>pip</code>. Please go to the quick start page.</p>"},{"location":"installation/other/nvidia_drivers/#installing-an-nvidia-driver","title":"Installing an Nvidia Driver","text":"<p>In general, this installation process explains how to install the Nvidia drivers for any version of IGM (and subsequently tensorflow). The documentation may be out of date in regards to the specific versions but the steps are the same. If you are looking for a quick installation and do not care about the details, you can simply attempt to run the following lines of code, in sequence, (for IGM 2.2.2 - and thus tensorflow 2.15.0). If you have any issues and still can not install the Nvidia drivers properly, please read the rest of the guide.</p> <pre><code>sudo apt update\nsudo apt upgrade\nsudo apt install nvidia-driver-535\nsudo reboot\n</code></pre>"},{"location":"installation/other/nvidia_drivers/#finding-potential-nvidia-drivers","title":"Finding potential Nvidia Drivers","text":"<p>If you do not have an existing Nvidia driver, you can install it directly onto your linux system. To do so, we first make sure we have the updated libraries from apt</p> <pre><code>sudo apt update\nsudo apt upgrade\n</code></pre> <p>Next, we can download a specific nvidia driver version. These versions depend on your GPU model, so you may see a different output than what is shown below. In any case, we first need to see which driver versions are compatable with our model (for more info, visit here); to do this, we can run the following command</p> <pre><code>sudo ubuntu-drivers list\n</code></pre> <p>If you have an available device, you will see a list of possible drivers like so <pre><code>nvidia-driver-535, (kernel modules provided by linux-modules-nvidia-535-generic-hwe-22.04)\nnvidia-driver-545-open, (kernel modules provided by nvidia-dkms-545-open)\nnvidia-driver-545, (kernel modules provided by nvidia-dkms-545)\nnvidia-driver-560-open, (kernel modules provided by nvidia-dkms-560-open)\nnvidia-driver-570, (kernel modules provided by nvidia-dkms-570)\nnvidia-driver-560-server, (kernel modules provided by nvidia-dkms-560)\n...\n</code></pre></p>"},{"location":"installation/other/nvidia_drivers/#selecting-an-nvidia-driver","title":"Selecting an Nvidia Driver","text":"<p>Now, to choose the correct nvidia driver, it will be constrained by the tensorflow version you would like to download. For instance, if IGM currently uses tensorflow 2.15 (you can always check the version of tensorflow in the setup.py file here), you will need a minimum version of CUDA and cuDNN (as shown here). From the table, we can see that we need a minimum cuDNN and CUDA version of 8.9 and 12.2, respectively.</p> Version Python version Compiler Build tools cuDNN CUDA tensorflow-2.15.0 3.9-3.11 Clang 16.0.0 Bazel 6.1.0 8.9 12.2 tensorflow-2.14.0 3.9-3.11 Clang 16.0.0 Bazel 6.1.0 8.7 11.8 <p>This means that we should install a driver that supports a CUDA version of at least 12.2. To check this we can check the minimum required driver version for both linux as well as windows (not wsl-windows) here. You will now find that for tensorflow 2.15, which requires a CUDA version of 12.2, we need a minimum driver version of <code>525.60.13</code>.</p> CUDA Toolkit Minimum Required Driver Version for CUDA Minor Version Compatibility* Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 12.x &gt;=525.60.13 &gt;=528.33 CUDA 11.8.x &gt;=450.80.02 &gt;=452.39"},{"location":"installation/other/nvidia_drivers/#installing-the-new-nvidia-driver","title":"Installing the new Nvidia Driver","text":"<p>Now, we can install the required driver (ideally not just the minimum version but the latest version). A smaller note is that it is recommended to install the base driver version rather than the <code>open</code> or <code>server</code> versions.</p> <pre><code>sudo apt install nvidia-driver-570 # this is the latest version at the moment\nsudo reboot # you wont see the changes until after you reboot\n</code></pre> <p>After rebooting, you can check your driver version with the <code>nvidia-smi</code> command and should see the new changes.</p>"},{"location":"installation/other/virtual_environment/","title":"Virtual Environments","text":"<p>To use IGM, it is recommended to use a virtual environment. This is because within IGM there exists specific versions of not only Tensorflow but many other packages dowloaded from PyPi and Conda. In short, to properly manage conflicts, we suggest installing IGM within a virtual environment. To do this, there exist a few different options. If you just want to install a basic virtual environment, then using Python could be the simplest option. However, we recommend using Conda for more complex projects where you might have different versions of packages. Both methods are illustrated below.</p>"},{"location":"installation/other/virtual_environment/#using-conda","title":"Using Conda","text":"<p>To create a virtual environment with conda, we first must install anaconda. The script below serves as an example for Linux, but one can also install it on windows. However, please note that IGM, and by consequence Tensorflow, as issues with native windows, so we recommend using either Linux or WSL windows. </p> <p>First, we install anaconda for our machine architecture by going to the archive. For example, this is for a Linux x86 64 bit machine.</p> <pre><code>wget https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh\nbash Anaconda3-2023.09-0-Linux-x86_64.sh\n</code></pre> <p>Next, we can create our virtual environment.</p> <p>Note</p> <p>Make sure to choose a python version that is in alignment with IGM's install requirements. You can check this by going here.</p> <pre><code>conda create --name igm python=3.10\n</code></pre> <p>Now, we can activate our environment so that we can install IGM.</p> <pre><code>conda activate igm\n</code></pre>"},{"location":"installation/other/virtual_environment/#using-python-virtual-environments","title":"Using Python Virtual Environments","text":"<p>Alternatively, one can install a virtual environment without needing to use conda by using the <code>venv</code> module that is built-in to <code>python</code>. This method is less flexible for complex environments but is easier to setup as you only need python installed.</p> <p>Simularly, we need to pick a correct version of python that is in line with the current IGM one wants to download.</p> <p>Then, using that python version, we can create a virtual environment</p> <pre><code>python3.10 -m venv igm\n</code></pre> <p>Lastly, we can activate this environment and install IGM.</p> <pre><code>source igm/bin/activate\n</code></pre>"},{"location":"installation/other/wsl_windows/","title":"WSL Windows","text":"<p>As Tensorflow does not have GPU support for native windows, it is recommended to use WSL windows, which is essentially a way to use Linux on a windows machine. You must do this step before proceeding to installing the Nvidia drivers and ultimately IGM.</p>"},{"location":"installation/other/wsl_windows/#installing-wsl","title":"Installing WSL","text":"<p>To get started, we first install wsl (more info here).</p> <pre><code>wsl --install Ubuntu-22.04\n</code></pre> <p>Then, we can proceed to install the NVIDIA drivers if you wish to use the GPU. At this step, you can go to the guide and follow the rest of the steps (installing the drivers and then IGM) as you are now using a Linux system.</p>"},{"location":"modules/core/","title":"Core","text":"<p>These core parameters are organized under a specific configuration structure designed to manage essential aspects of IGM's workflow, hardware preferences, and logging options. </p>"},{"location":"modules/core/#config-structure","title":"Config Structure","text":"<pre><code>core:\n  # Input / Output\n  url_data: \"\"\n  folder_data: data\n  logging_file: \"\"\n\n  # Logging\n  logging: False\n  logging_level: 30\n\n  # Hardware\n  hardware:\n    gpu_info: True\n    visible_gpus: [0]\n    strategy: none\n\n  # Misc\n  print_params: False\n  print_comp: False\n  print_info: True\n  print_imported_modules: True\n  check_compat_params: True\n\n  # Custom Modules\n  structure:\n    root_foldername: user\n    conf_foldername: conf\n    code_foldername: code\n    input_modules_foldername: inputs\n    process_modules_foldername: processes\n    output_modules_foldername: outputs\n</code></pre>"},{"location":"modules/core/#parameters","title":"Parameters","text":"Name Type Units Description Default Value url_data str \\( dimless \\) Url supplied to download .nc data files folder_data str \\( dimless \\) data data logging_file str \\( dimless \\) logging bool \\( dimless \\) False logging_level str \\( dimless \\) 30 30 hardware.gpu_info bool \\( dimless \\) True True hardware.visible_gpus list \\( dimless \\) visible gpus [0] hardware.strategy str \\( dimless \\) strategy none print_params bool \\( dimless \\) True False print_comp bool \\( dimless \\) Print information related to computational performances False print_info bool \\( dimless \\) Print live information when the model is running True print_imported_modules bool \\( dimless \\) Print imported modules True check_compat_params bool \\( dimless \\) Check compatibility btween default pasm name and user-defined one to check for typos, ect. True structure.root_foldername str \\( dimless \\) user user structure.conf_foldername str \\( dimless \\) conf conf structure.code_foldername str \\( dimless \\) code code structure.input_modules_foldername str \\( dimless \\) inputs inputs structure.process_modules_foldername str \\( dimless \\) processes processes structure.output_modules_foldername str \\( dimless \\) outputs outputs"},{"location":"modules/introduction/","title":"Introduction","text":"<p>IGM is organized in a modular fashion, reflecting the similarity in tasks such as loading data, initializing fields, updating these fields within a time loop, and outputting results at regular intervals. Each module is designed to handle a specific aspect of the glacier evolution process, ensuring a clear and modular structure:</p> <ul> <li> <p>The <code>inputs</code> modules are responsible for loading data, such as glacier bedrock and ice surface velocities.</p> </li> <li> <p>The <code>processes</code> modules implement physical mechanisms in a modular way, such as ice flow and mass conservation.</p> </li> <li> <p>The <code>outputs</code> modules handle writing or visualizing model results.</p> </li> </ul> <p>The main Python script <code>igm_run</code> is responsible for orchestrating the entire simulation process. It loads all <code>inputs</code>, <code>processes</code>, and <code>outputs</code> modules along with their parameters, initializes them, updates them within a time loop, and finalizes them. The majority of the IGM code resides within these modules, while the core structure remains minimal and lightweight. Each module includes:</p> <ul> <li> <p>a name that is used to identify it (e.g., <code>greatmodule</code>).</p> </li> <li> <p>a source code located in <code>XXX/greatmodule/greatmodule.py</code>, where <code>XXX</code> is the folder <code>inputs</code>, <code>processes</code>, or <code>outputs</code> depending on the module's purpose.</p> </li> <li> <p>a parameter file located in <code>conf/XXX/greatmodule.yaml</code> containing default parameters.</p> </li> <li> <p>a help file located in <code>conf_help/XXX/greatmodule.yaml</code> that defines each parameter, including its type and unit.</p> </li> <li> <p>a documentation page in the <code>igm-doc</code> GitHub repository, which is used in the documentation.</p> </li> </ul> <p>For convenience, users can create their own module-parameter pairs in a dedicated <code>user</code> folder. These user-defined modules can be referenced directly from the parameter file, enabling customization and flexibility to extend the simulation's functionality. This approach allows users to tailor the simulation to their specific needs without altering the core modules. Check section on user modules for more information.</p>"},{"location":"modules/introduction/#modular-structure-of-igm","title":"Modular Structure of IGM","text":""},{"location":"modules/user_modules/","title":"User modules","text":"<p>It is easy to create your own module in a separate Python file and integrate it into the workflow. For instance, you might want to implement a climate or surface mass balance model tailored to a specific application. To achieve this, it is crucial to understand the structure and operation of IGM. User modules follow the same structure as built-in ones, so you can use built-in modules as a reference or starting point when designing your own.</p>"},{"location":"modules/user_modules/#coding-structure","title":"Coding structure","text":"<p>A closer look at the main script <code>igm_run.py</code> reveals the following structure:</p> <ul> <li><code>run</code> all inputs modules</li> <li><code>initialize</code> all processes modules</li> <li><code>initialize</code> all outputs modules</li> </ul> <p>For all time steps:</p> <ul> <li><code>update</code> all processes modules</li> <li><code>run</code> all outputs modules</li> <li><code>finalize</code> all processes modules</li> </ul> <p>Here, we find that</p> <ul> <li><code>inputs</code> modules have a <code>run</code> function,</li> <li><code>processes</code> modules have <code>initialize</code>, <code>update</code>, and <code>finalize</code> functions</li> <li><code>outputs</code> modules have <code>initialize</code> and <code>run</code> functions.</li> </ul> <p>Similarly to existing IGM modules, a user-defined module <code>my_module</code> can be implemented and automatically loaded when <code>igm_run</code> is executed, provided that <code>my_module</code> and the path to its parameter file are correctly specified. Building a user module involves creating the following folder structure (folder <code>user</code> lies alongside <code>experiment</code> and <code>data</code>):</p> <pre><code>\u2514\u2500\u2500 user\n  \u251c\u2500\u2500 code\n  \u2502   \u2514\u2500\u2500 input\n  \u2502   \u2502   \u2514\u2500\u2500 my_module.py\n  \u2502   \u2514\u2500\u2500 processes\n  \u2502   \u2502   \u2514\u2500\u2500 my_module.py\n  \u2502   \u2514\u2500\u2500 outputs\n  \u2502       \u2514\u2500\u2500 my_module.py\n  \u2514\u2500\u2500 conf\n    \u2514\u2500\u2500 input\n    \u2502   \u2514\u2500\u2500 my_module.yaml\n    \u2514\u2500\u2500 processes\n    \u2502   \u2514\u2500\u2500 my_module.yaml\n    \u2514\u2500\u2500 outputs\n      \u2514\u2500\u2500 my_module.yaml\n</code></pre> <p>Here, the code files are expected to define functions <code>run(cfg, state)</code>, <code>initialize(cfg, state)</code>, <code>update(cfg, state)</code>, and/or <code>finalize(cfg, state)</code>, where</p> <ul> <li> <p>the <code>cfg</code> object allows access to parameters in a hierachical fashion (e.g., <code>cfg.processes.enthalpy.ref_temp</code> retrieves a parameter associated with the <code>enthalpy</code> processes module),</p> </li> <li> <p>the <code>state</code> object provides access to variables describing the glacier state at a given time <code>t</code> (e.g., <code>state.thk</code> represents the distributed 2D ice thickness). All these variables are TensorFlow 2.0 Tensors. Leveraging TensorFlow is essential for performing computationally efficient operations, particularly on GPUs (see the dedicated TensorFlow section below). Variables can be accessed or modified using <code>state.name_of_the_variable</code>. Check at the section below to know more how to code in Tensorflow.</p> </li> </ul>"},{"location":"modules/user_modules/#example","title":"Example","text":"<p>To implement a mass balance function <code>sinus</code> with an oscillating ELA, you may create a module <code>mysmb</code> in a file <code>mysmb.py</code>:</p> <pre><code>def initialize(cfg,state):\n    pass\n\ndef update(cfg,state): \n    ELA = cfg.processes.mysmb.meanela + \\\n          750*math.sin((state.t/100)*math.pi) \n    state.smb  = state.usurf - ELA\n    state.smb *= tf.where(state.smb&lt;0, 0.005, 0.009)\n    state.smb  = tf.clip_by_value(state.smb, -100, 2) \n\ndef finalize(cfg,state):\n    pass\n</code></pre> <p>and a parameter file <code>mysmb.yaml</code> containing the default value:</p> <pre><code>mysmb:\n  meanela: 3200\n</code></pre> <p>Then, in the parameter file <code>params.yaml</code>, you need to:</p> <ol> <li>List the user module so that the code will be found.</li> <li>Include the parameter file so that its parameters are added to the existing ones.</li> </ol> <p>Here is an example of how to modify <code>params.yaml</code>:</p> <p><pre><code> @package _global_\n\ndefaults:\n\n  - /user/conf/processes@processes.mysmb: mysmb\n\n  - override /processes:  \n     - mysmb\n     - iceflow\n     - time\n     - thk \n  ...\n</code></pre> Note that the three functions (<code>initialize</code>, <code>update</code>, <code>finalize</code>) must be defined, even if some do not perform any operations (in such cases, simply use <code>pass</code>). For inspiration or examples, you can refer to the code of existing IGM modules.</p>"},{"location":"modules/user_modules/#overriding-a-module","title":"Overriding a module","text":"<p>Sometimes, you may need to modify an existing built-in module. This can be achieved by creating a user module that overrides the built-in functionality. To do this, define a module with the same name as the existing module (e.g., <code>existingmodule.py</code>) and implement the desired customizations. </p> <p>For example, the <code>aletsch-1880-2100</code> module implements a custom seeding strategy for the particle module. This is done by defining a user-specific <code>particles.py</code> file, which overrides the built-in functions. Here is an example:</p> <pre><code>#!/usr/bin/env python3\nimport igm \n\n# Take over the official functions\nupdate   = igm.processes.particles.particles.update\nfinalize = igm.processes.particles.particles.finalize\n\n# Define a new initialize function using the official one\ndef initialize(cfg, state):\n  igm.processes.particles.particles.initialize(cfg, state)\n  [...] # Load the custom seeding map\n\n# Customize the seeding_particles function\ndef seeding_particles(cfg, state):\n  [...] # Implement the custom seeding logic\n\n# Override the official seeding_particles function\nigm.processes.particles.particles.seeding_particles = seeding_particles\n</code></pre> <p>By following this approach, you can surgically extend or modify the behavior of existing modules while preserving the original functionality. This ensures flexibility and adaptability for specific use cases without compromising the integrity of the built-in modules.</p>"},{"location":"modules/user_modules/#tensorflow","title":"Tensorflow","text":"<p>IGM relies on the TensorFlow 2.0 library to achieve computational efficiency, particularly on GPUs. All variables, such as ice thickness, are represented as TensorFlow tensor objects. These tensors can only be modified using TensorFlow operations, which are inherently vectorized. This vectorization allows operations to be applied simultaneously across all entries of 2D gridded fields, enabling parallel and efficient execution.</p> <p>To maximize performance, avoid sequential operations, such as loops over indices of 2D arrays. Instead, leverage TensorFlow's optimized operations designed for large arrays, which are commonly used in machine learning and neural networks. By adhering to this approach, you can ensure that your computations remain efficient and fully utilize the capabilities of TensorFlow.</p> <p>At first glance, many TensorFlow functions resemble those in NumPy. For example, you can perform operations by replacing NumPy with TensorFlow, such as using <code>tf.zeros()</code> instead of <code>np.zeros()</code>. Additionally, you would import TensorFlow as <code>import tensorflow as tf</code> instead of <code>import numpy as np</code>. Here is an example of TensorFlow operations:</p> <pre><code>import tensorflow as tf\n\n# Create a tensor filled with zeros\ntensor = tf.zeros((500, 300))\n\n# Perform operations on the tensor\ntensor = (2 * tensor + 200) ** 2\n</code></pre> <p>While the syntax may appear similar, TensorFlow is specifically optimized for GPU acceleration and large-scale computations, making it more suitable for high-performance tasks compared to NumPy. This optimization allows TensorFlow to handle operations on large datasets efficiently, leveraging parallel processing capabilities of modern hardware.</p> <pre><code>state.topg  = tf.zeros_like(state.usurf)                                  # define Variable Tensor\nstate.smb   = tf.where(state.usurf &gt; 4000, 0, state.smb)                   # Imposes zero mass balance above 4000 m asl.\nstate.usurf = state.topg + state.thk                                       # Update surface topography with new ice thickness\nstate.smb   = tf.clip_by_value( (state.usurf - ela)*grad , -100, 2.0 )     # Define linear smb wrt z, with capping value\nu = tf.concat( [u[:, 0:1], 0.5 * (u[:, :-1] + u[:, 1:]), u[:, -1:]], 1 )   # work on straggered grid\n</code></pre> <p>In fact, there are two kinds of tensors used in IGM. The first type is the \"EagerTensor,\" which supports many operations but does not allow modification of specific tensor entries (slicing). For example:</p> <pre><code>tensor = tf.ones((500,300))  \ntensor = (2*tensor + 200)**2\ntensor[1,2] = 5 # WILL NOT WORK\n</code></pre> <p>As a workaround, you can use \"tf.Variable,\" which allows slicing. However, assignments must be performed using the <code>assign</code> function instead of the <code>=</code> operator:</p> <pre><code>tensor = tf.Variable(tf.ones((500, 300)))\ntensor.assign((2 * tensor + 200) ** 2)\ntensor[1, 2].assign(5)  # WORKS!\n</code></pre> <p>IGM combines both types of tensors, so it is essential to identify the type of tensor you are working with. Otherwise, TensorFlow will produce an error.</p> <p>For optimal computational efficiency, it is crucial to keep all variables and operations within the TensorFlow framework and avoid using NumPy. This prevents unnecessary data transfers between GPU and CPU memory. The best way to learn how to code with TensorFlow in the context of IGM is to explore the existing IGM module code.</p>"},{"location":"modules/user_modules/#sharing-your-module","title":"Sharing your module","text":"<p>If you have developed a module that you believe could benefit the community and be included in the IGM package, please reach out to the IGM development team. </p>"},{"location":"modules/inputs/load_ncdf/","title":"Module <code>load_ncdf</code>","text":"<p>Warning: we advise to use instead module <code>local</code></p> <p>This IGM module is designed to load spatial 2D raster data from a NetCDF file specified by the <code>input_file</code> parameter. The module converts all existing 2D fields into TensorFlow variables. At a minimum, the module is expected to import basal topography represented by the <code>topg</code> variable. Additionally, it completes the data, such as deriving basal topography from ice thickness and surface topography. Other fields present in the NetCDF file will also be converted to TensorFlow variables, allowing them to be accessed in the code via <code>state.myvar</code>. For example, providing the <code>icemask</code> variable can be useful in defining an accumulation area, which is beneficial for modeling individual glaciers and preventing overflow into neighboring catchments.</p> <p>The module offers functions for resampling the data, where the <code>coarsen</code> parameter can be set to values like 2, 3, or 4 (with a default value of 1 indicating no coarsening). It also provides functionality for cropping the data by setting the <code>crop</code> parameter to <code>True</code> and specifying the desired bounds.</p> <p>Additionally, by setting <code>icemask_invert</code> to <code>True</code>, an ice mask can be generated from an ESRI Shapefile specified by the <code>icemask_shapefile</code> parameter. This mask can identify areas that should contain glaciers or areas that should remain glacier-free, based on the <code>icemask_include</code> parameter.</p> <p>The module also supports restarting an IGM run using a NetCDF file produced from a previous IGM run. To achieve this, provide the output NetCDF file from the previous run as input to IGM. The module will seek data corresponding to the starting time defined by <code>processes.time.start</code> and initialize the simulation at that time.</p> <p>This module depends on <code>netCDF4</code>.</p> <p>Contributors: G. Jouvet, A. Henz (icemask add-on)</p>"},{"location":"modules/inputs/load_ncdf/#config-structure","title":"Config Structure","text":"<pre><code>load_ncdf:\n  input_file: input.nc\n  method_coarsen: skipping\n  coarsen: 1\n  crop: False\n  xmin: -1.0e+20\n  xmax: 1.0e+20\n  ymin: -1.0e+20\n  ymax: 1.0e+20\n  icemask_include: False\n  icemask_shapefile: icemask.shp\n  icemask_invert: False\n</code></pre>"},{"location":"modules/inputs/load_ncdf/#parameters","title":"Parameters","text":"Name Type Units Description Default Value input_file str dimless NetCDF input data file input.nc method_coarsen str dimless Method for coarsening the data from NetCDF file (skipping or cubic_spline) skipping coarsen int dimless Coarsen the data from NetCDF file by a certain (integer) number (2 would be twice coarser ignore data each 2 grid points) 1 crop bool dimless Crop the data from NetCDF file with given top/down/left/right bounds False xmin float m X left coordinate for cropping the NetCDF data -1e+20 xmax m float X right coordinate for cropping the NetCDF data 1e+20 ymin float m Y bottom coordinate for cropping the NetCDF data -1e+20 ymax float m Y top coordinate for cropping the NetCDF data 1e+20 icemask_include bool dimless Include ice mask in the NetCDF file False icemask_shapefile str dimless Shapefile for ice mask icemask.shp icemask_invert bool dimless Invert the ice mask False <p>--&gt;</p>"},{"location":"modules/inputs/load_ncdf/#example-usage","title":"Example Usage","text":""},{"location":"modules/inputs/load_tif/","title":"Module <code>load_tif</code>","text":"<p>Warning: we advise to use instead module <code>local</code></p> <p>This IGM module is designed to load spatial 2D raster data from any <code>.tif</code> file present in the working directory (<code>folder</code>) and transform each of them into TensorFlow variables. The name of the file becomes the name of the variable. For example, the file <code>topg.tif</code> will yield the variable <code>topg</code>. At a minimum, the module is expected to import basal topography represented by the <code>topg</code> variable. Additionally, it can derive basal topography from ice thickness and surface topography. Other fields present in the folder will also be converted to TensorFlow variables, allowing them to be accessed in the code via <code>state.myvar</code>. For instance, providing the <code>icemask</code> variable can help define an accumulation area, which is useful for modeling individual glaciers and preventing overflow into neighboring catchments.</p> <p>The module provides functionality for resampling the data using the <code>coarsen</code> parameter, which can be set to values like 2, 3, or 4 (with a default value of 1 indicating no coarsening). It also supports cropping the data by setting the <code>crop</code> parameter to <code>True</code> and specifying the desired bounds.</p> <p>Additionally, by setting <code>icemask_invert</code> to <code>True</code>, an ice mask can be generated from an ESRI Shapefile specified by the <code>icemask_shapefile</code> parameter. This mask can identify areas that should contain glaciers or remain glacier-free, based on the <code>icemask_include</code> parameter.</p> <p>This module depends on <code>rasterio</code>.</p> <p>Contributors: G. Jouvet, A. Henz (icemask add-on)</p>"},{"location":"modules/inputs/load_tif/#config-structure","title":"Config Structure","text":"<pre><code>load_tif:\n  coarsen: 1\n  crop: False\n  xmin: -1.0e+20\n  xmax: 1.0e+20\n  ymin: -1.0e+20\n  ymax: 1.0e+20\n  icemask_include: False\n  icemask_shapefile: icemask.shp\n  icemask_invert: False\n</code></pre>"},{"location":"modules/inputs/load_tif/#parameters","title":"Parameters","text":"Name Type Units Description Default Value coarsen int dimless Coarsen the data from NetCDF file by a certain (integer) number (2 would be twice coarser ignore data each 2 grid points) 1 crop bool dimless Crop the data from NetCDF file with given top/down/left/right bounds False xmin float m Crop the data from NetCDF file with the given minimum x bound -1e+20 xmax m float Crop the data from NetCDF file with the given maximum x bound 1e+20 ymin float m Crop the data from NetCDF file with the given minimum y bound -1e+20 ymax float m Crop the data from NetCDF file with the given maximum y bound 1e+20 icemask_include bool dimless Include ice mask in the NetCDF file False icemask_shapefile str dimless Shapefile for ice mask icemask.shp icemask_invert bool dimless Invert the ice mask False"},{"location":"modules/inputs/load_tif/#example-usage","title":"Example Usage","text":""},{"location":"modules/inputs/local/","title":"Module <code>local</code>","text":"<p>This IGM module is designed to load spatial 2D raster data from NetCDF or Tiff files specified by the <code>input_file</code> parameter. The module converts all existing 2D fields (present in the NetCDF file or in the designated folder for Tiff files) into TensorFlow variables. At a minimum, the module is expected to import basal topography represented by the <code>topg</code> variable. Additionally, it completes the data by deriving basal topography from ice thickness and surface topography when necessary. Other fields present will also be converted to TensorFlow variables, allowing them to be accessed in the code via <code>state.myvar</code>. For example, providing the <code>icemask</code> variable can be useful for defining an accumulation area, which is beneficial for modeling individual glaciers and preventing overflow into neighboring catchments.</p> <p>The module offers functions for resampling the data, where the <code>coarsen</code> parameter can be set to values like 2, 3, or 4 (with a default value of 1 indicating no coarsening). It also supports cropping the data by setting the <code>crop</code> parameter to <code>True</code> and specifying the desired bounds.</p> <p>Additionally, by setting <code>icemask_invert</code> to <code>True</code>, an ice mask can be generated from an ESRI Shapefile specified by the <code>icemask_shapefile</code> parameter. This mask can identify areas that should contain glaciers or areas that should remain glacier-free, based on the <code>icemask_include</code> parameter.</p> <p>The module also supports restarting an IGM run using a NetCDF file produced from a previous IGM run. To achieve this, provide the output NetCDF file from the previous run as input to IGM. The module will seek data corresponding to the starting time defined by <code>processes.time.start</code> and initialize the simulation at that time.</p> <p>This module depends on the <code>xarray</code> and <code>rioxarray</code> Python libraries.</p> <p>Contributors: B. Finley, A. Henz (icemask add-on), G. Jouvet (tif format)</p>"},{"location":"modules/inputs/local/#config-structure","title":"Config Structure","text":"<pre><code>local:\n  type: netcdf\n  filename: input.nc\n  coarsening:\n    boundary: trim\n    ratio: 1\n  crop:\n    xmin: null\n    xmax: null\n    ymin: null\n    ymax: null\n  icemask:\n    include: False\n    shapefile: icemask.shp \n    invert: False\n</code></pre>"},{"location":"modules/inputs/local/#parameters","title":"Parameters","text":"Name Type Units Description Default Value type str \\( dimless \\) Type of file to be loaded, can be netcdf or tif netcdf filename str \\( dimless \\) NetCDF input data file input.nc coarsening.boundary str \\( dimless \\) Method for coarsening the data from NetCDF file: skipping or cubic_spline trim coarsening.ratio float \\( dimless \\) Coarsen ratio 1 crop.xmin float \\( m \\) X left coordinate for cropping the NetCDF data None crop.xmax float \\( m \\) X right coordinate for cropping the NetCDF data None crop.ymin float \\( m \\) Y bottom coordinate for cropping the NetCDF data None crop.ymax float \\( m \\) Y top coordinate for cropping the NetCDF data None icemask.include bool \\( dimless \\) Include ice mask in the NetCDF file False icemask.shapefile str \\( dimless \\) Shapefile for ice mask icemask.shp icemask.invert bool \\( dimless \\) Invert the ice mask False"},{"location":"modules/inputs/oggm_shop/","title":"Module <code>oggm_shop</code>","text":"<p>This IGM module utilizes OGGM utilities and the GlaThiDa dataset to prepare data for the IGM model for a specific glacier given its RGI ID (parameter <code>RGI_ID</code>). You can check the GLIMS Viewer to find the RGI ID of your glacier. By default, IGM references RGI 7.0C, which can be found here. Altering the <code>RGI_product</code> parameter ('C' for glacier complex or 'G' for individual glacier) allows you to switch between these options. By default, data is preprocessed (<code>preprocess</code> parameter is set to True) with a spatial resolution of 100 meters and a border size of 30 meters. To customize the spatial resolution and the size of the 'border' to maintain a safe distance from the glacier margin, you need to set the <code>preprocess</code> parameter to False, and then set the <code>dx</code> and <code>border</code> parameters as desired. </p> <p>When this module is executed, it automatically downloads a suite of data (refer to the table above) associated with the specified glacier (individual or complex). These data are stored in a directory named after the RGI ID within the <code>data</code> folder. Subsequently, a file named <code>input.nc</code> is created, containing the relevant variables renamed according to IGM's naming conventions.</p> <p>Important: The <code>oggm_shop</code> module must be followed by the <code>local</code> inputs module, which reads <code>data/input.nc</code> and loads the 2D gridded variables as TensorFlow objects in IGM.</p> <p>If IGM is run a second time, it will not re-download the OGGM data or recreate <code>data/input.nc</code> as long as the data remain in the <code>data</code> folder. This is particularly useful for parameter analysis, as it avoids redundant data downloads during multiple runs. The <code>oggm_shop</code> module provides all the necessary data variables required to run IGM for forward glacier evolution simulations, assuming the availability of basal topography (<code>topg</code>) and ice thickness (<code>thk</code>). </p> <p>Additionally, it supports preliminary data assimilation and inverse modeling tasks within the <code>iceflow</code> module. These tasks typically involve variables such as <code>icemaskobs</code>, <code>thkinit</code>, <code>uvelsurf</code>, <code>vvelsurf</code>, <code>thkobs</code>, and <code>usurfobs</code>.</p> <p>Data available via the shop are listed in the table below. Users can specify the source for ice thickness using the <code>thk_source</code> parameter (options: <code>millan_ice_thickness</code> or <code>consensus_ice_thickness</code> dataset) and for ice velocities using the <code>vel_source</code> parameter (options: <code>millan_ice_velocity</code> or <code>its_live</code> dataset).</p> <p>Set <code>include_glathida</code> to True to retrieve the GlaThiDa ice thickness profiles for data assimilation purpose. When using RGI 6.0, these profiles are sourced from the GlaThiDa repository. For RGI 7.0, GlaThiDa data specific to the glacier, identified by its RGI ID, are retrieved from the OGGM server. These data are saved as a text file named <code>glathida_data.csv</code> in the module's download directory. In both cases, the data are read, rasterized with the label <code>thkobs</code>, and any pixels lacking observations are marked with NaN values.</p> Variable Reference Ice thickness profile GlaThiDa ----------------------- ---------------------------------------- Surface DEM Copernicus DEM GLO-90 Ice thickness Millan et al. (2022) Ice thickness Farinotti et al. (2019) Surface ice speeds Millan et al. (2022) Surface ice speeds its-live.jpl.nasa.gov Glacier mask Randolph Glacier Inventory Ice thickness profile GlaThiDa Glacier change Hugonnet et al. (2021) Climate data GSWP3_W5E5 Flowline OGGM <p>Table: Products available with the <code>oggm_shop</code> module.</p> <p>The module depends (of course) on the <code>oggm</code> library. Unfortunately the module works on linux and Max, but not on windows (unless using WSL).</p> <p>Contributors: F. Maussion, G. Jouvet, E. Welty (GlaThiDa-related code), S. Cook (RGI 7.0 modifications).</p>"},{"location":"modules/inputs/oggm_shop/#config-structure","title":"Config Structure","text":"<pre><code>oggm_shop:\n  RGI_ID: \"RGI60-11.01450\"\n  RGI_IDs: [\"RGI60-11.01450\"]\n  preprocess: True\n  RGI_version: 7\n  dx: 100\n  border: 30\n  thk_source: consensus_ice_thickness\n  vel_source: millan_ice_velocity\n  incl_glathida: False\n  path_glathida: \"\"\n  filename: input.nc\n  remove_RGI_folder: True\n  sub_entity_mask: False\n  RGI_product: \"G\"\n  run_batch: False\n  smooth_obs_vel: True\n  highres: False\n</code></pre>"},{"location":"modules/inputs/oggm_shop/#parameters","title":"Parameters","text":"Name Type Units Description Default Value RGI_ID str dimless RGI ID of the selected glacier, G stands for individual Glacier, and C for glacier Complex for RGI7 RGI60-11.01450 RGI_IDs list dimless List of RGI IDs of the selected glaciers, G stands for individual Glacier, and C for glacier Complex for RGI7 ['RGI60-11.01450'] preprocess bool dimless Use OGGM preprocessing if you are fine with given spatial resolution and border size True RGI_version int dimless RGI versio : 6 or 7 7 dx float m Spatial resolution (need preprocess false to change it) 100 border int dimless Safe border margin, number of pixels, (need preprocess false to change it) 30 thk_source str dimless Millan ice thickness (millan_ice_thickness) or consensus ice thickness (consensus_ice_thickness) consensus_ice_thickness vel_source str dimless Source of the surface velocities (millan_ice_velocity or its_live) millan_ice_velocity incl_glathida bool dimless Set this to True for retrieving the GlaThiDa ice thickness profiles for the inversion. False path_glathida str dimless Path where the Glathida Folder is stored, so that you don't need to redownload it at every use of the script, if empty it will be in the home directory (only relevant for RGI6.0) filename str dimless NetCDF input data file created by OGGM input.nc remove_RGI_folder bool dimless Remove RGI folder True sub_entity_mask bool dimless Ice mask shows individual RGI 7.0G entities within each larger RGI 7.0C complex False RGI_product str dimless Glacier complexes (C) or individual basins (G) (default is G, individual basins) G run_batch bool dimless Run all the glaciers in the world False smooth_obs_vel bool dimless Smooth the observed velocities True highres bool dimless Use high resolution data False"},{"location":"modules/inputs/oggm_shop/#example-usage","title":"Example Usage","text":""},{"location":"modules/outputs/local/","title":"Module <code>local</code>","text":"<p>This IGM module writes 2D field variables defined in the parameter list <code>vars_to_save</code> into the NetCDF output file specified by the parameter <code>output_file</code> (default: <code>output.nc</code>). The saving frequency is determined by the parameter <code>processes.time.save</code> defined in the <code>time</code> module.</p> <p>This module depends on <code>xarray</code>.</p>"},{"location":"modules/outputs/local/#config-structure","title":"Config Structure","text":"<pre><code>local:\n  file_format_list: ['netcdf']\n  output_file: output.nc\n  vars_to_save:\n    - topg\n    - usurf\n    - thk\n    - smb\n    - velbar_mag\n    - velsurf_mag\n    - uvelsurf\n    - vvelsurf\n    - wvelsurf\n  write_ts: True\n  output_ts_file: output_ts.nc\n</code></pre>"},{"location":"modules/outputs/local/#parameters","title":"Parameters","text":"Name Type Units Description Default Value file_format_list list dimless List of file formats to be used for output (e.g., ['netcdf','tif']) ['netcdf'] output_file str dimless Output ncdf data file output.nc vars_to_save list dimless List of variables to be recorded in the ncdf file ['topg', 'usurf', 'thk', 'smb', 'velbar_mag', 'velsurf_mag', 'uvelsurf', 'vvelsurf', 'wvelsurf'] write_ts bool dimless Flag to write time series data True output_ts_file str dimless Output ncdf time series data file output_ts.nc"},{"location":"modules/outputs/local/#example-usage","title":"Example Usage","text":""},{"location":"modules/outputs/plot2d/","title":"Module <code>plot2d</code>","text":"<p>This IGM module generates 2D plan-view plots of a variable specified by the parameter <code>var</code> (e.g., <code>var</code> can be set to <code>thk</code>, <code>ubar</code>, etc.). The saving frequency is determined by the parameter <code>processes.time.save</code> defined in the <code>time</code> module. The color bar's scale range is controlled by the parameter <code>varmax</code>.</p> <p>By default, the plots are saved as PNG files in the working directory. However, you can display the plot \"live\" by setting <code>live</code> to <code>True</code>. </p> <p>If the <code>particles</code> module is activated, you can overlay particles on the plot by setting <code>particles</code> to <code>True</code>, or exclude them by setting it to <code>False</code>.</p>"},{"location":"modules/outputs/plot2d/#config-structure","title":"Config Structure","text":"<pre><code>plot2d:\n  editor: vs\n  live: False\n  particles: True\n  var: \"velbar_mag\"\n  var_max: 1000\n</code></pre>"},{"location":"modules/outputs/plot2d/#parameters","title":"Parameters","text":"Name Type Units Description Default Value editor str dimless Optimized for VS code (vs) or spyder (sp) for live plot vs live bool dimless Display plots live the results during computation instead of making png False particles bool dimless Display particles is True, does not display if False True var str dimless Name of the variable to plot velbar_mag var_max float ['variable'] Maximum value of the varplot variable used to adjust the scaling of the colorbar 1000"},{"location":"modules/outputs/plot2d/#example-usage","title":"Example Usage","text":""},{"location":"modules/outputs/write_ncdf/","title":"Module <code>write_ncdf</code>","text":"<p>This IGM module writes 2D field variables specified in the parameter list <code>vars_to_save</code> into the NetCDF output file defined by the parameter <code>output_file</code> (default: <code>output.nc</code>). The saving frequency is determined by the parameter <code>processes.time.save</code> in the <code>time</code> module.</p> <p>This module requires the <code>netCDF4</code> library.</p>"},{"location":"modules/outputs/write_ncdf/#config-structure","title":"Config Structure","text":"<pre><code>write_ncdf:\n  output_file: output.nc\n  vars_to_save:\n    - topg\n    - usurf\n    - thk\n    - smb\n    - velbar_mag\n    - velsurf_mag\n    - uvelsurf\n    - vvelsurf\n    - wvelsurf\n</code></pre>"},{"location":"modules/outputs/write_ncdf/#parameters","title":"Parameters","text":"Name Type Units Description Default Value output_file str dimless Output ncdf data file output.nc vars_to_save list dimless List of variables to be recorded in the ncdf file ['topg', 'usurf', 'thk', 'smb', 'velbar_mag', 'velsurf_mag', 'uvelsurf', 'vvelsurf', 'wvelsurf']"},{"location":"modules/outputs/write_ncdf/#example-usage","title":"Example Usage","text":""},{"location":"modules/outputs/write_tif/","title":"Module <code>write_tif</code>","text":"<p>This IGM module writes 2D field variables listed in the parameter <code>vars_to_save</code> into TIFF output files. The files are named using the variable name and the time step (e.g., <code>thk-000040.tif</code>, <code>usurf-000090.tif</code>) and are saved in the working directory. The saving frequency is determined by the parameter <code>processes.time.save</code> defined in the <code>time</code> module. </p> <p>If the input files were loaded using the <code>load_tif</code> module, the TIFF metadata is preserved and included in the output files.</p> <p>This module requires the <code>rasterio</code> library.</p>"},{"location":"modules/outputs/write_tif/#config-structure","title":"Config Structure","text":"<pre><code>write_tif:\n  vars_to_save:\n      - usurf\n      - thk\n</code></pre>"},{"location":"modules/outputs/write_tif/#parameters","title":"Parameters","text":"Name Type Units Description Default Value vars_to_save list dimless List of variables to be saved as geotiff files ['usurf', 'thk']"},{"location":"modules/outputs/write_tif/#example-usage","title":"Example Usage","text":""},{"location":"modules/outputs/write_ts/","title":"Module <code>write_ts</code>","text":"<p>This module writes time series variables, such as ice-glaciated area and volume, into the NetCDF output file specified by the <code>output_file</code> parameter (default: <code>output_ts.nc</code>). The saving frequency is determined by the <code>processes.time.save</code> parameter defined in the <code>time</code> module.</p>"},{"location":"modules/outputs/write_ts/#config-structure","title":"Config Structure","text":"<pre><code>write_ts:\n  output_file: output_ts.nc\n</code></pre>"},{"location":"modules/outputs/write_ts/#parameters","title":"Parameters","text":"Name Type Units Description Default Value output_file str dimless Output ncdf data file (time series) output_ts.nc"},{"location":"modules/outputs/write_ts/#example-usage","title":"Example Usage","text":""},{"location":"modules/processes/avalanche/","title":"Module <code>avalanche</code>","text":"<p>This IGM module simulates the redistribution of snow and ice due to gravitational avalanches. The model assumes that when the local surface slope exceeds a specified threshold (angle of repose), mass is redistributed toward lower elevations until the surface slope is reduced below this threshold.</p> <p>Contributors: This function was adapted from Mark Kessler's GC2D program and implemented in IGM by J\u00fcrgen Mey with support from Guillaume Jouvet.</p>"},{"location":"modules/processes/avalanche/#config-structure","title":"Config Structure","text":"<pre><code>avalanche:\n  update_freq: 1\n  angleOfRepose: 30.0\n</code></pre>"},{"location":"modules/processes/avalanche/#parameters","title":"Parameters","text":"Name Type Units Description Default Value update_freq float y Frequency at which to update the avalanche module 1 angleOfRepose float Degrees Angle of repose (threshold for avalanche initiation) 30.0"},{"location":"modules/processes/avalanche/#example-usage","title":"Example Usage","text":"<p>We can run a simulation with a higher frequency of avalanches by changing the <code>processes.avalanche.update_freq</code> argument. We can either do this in our config file.</p> <pre><code># @package _global_\n\ninputs:\n  load_ncdf:\n    input_file: data/input.nc\n\nprocesses:\n  smb_simple:\n    array:\n      - [\"time\", \"gradabl\", \"gradacc\", \"ela\", \"accmax\"]\n      - [1900, 0.009, 0.005, 2800, 2.0]\n      - [2000, 0.009, 0.005, 2900, 2.0]\n      - [2100, 0.009, 0.005, 3300, 2.0]\n  time:\n    start: 1900.0\n    end: 2000.0\n    save: 10.0\n  avalanche:\n      update_freq: 5 # every 5 years\n</code></pre> <p>Alternatively, we can do it over the command line</p> <pre><code>igm_run +experiment/params processes.avalanche.update_freq=5\n</code></pre>"},{"location":"modules/processes/clim_glacialindex/","title":"Module <code>clim_glacialindex</code>","text":"<p>Module <code>clim_glacialindex</code> loads two climate snapshots (associated with certain periods) and interpolates them using a climate signal and a glacial index approach [1]. It is suitable for paleo-glacier modeling.</p> <p>For this purpose, we define a function GI (Glacier Index) that maps time \\(t\\) to a scalar with two extreme states: one state \\({\\rm CL}_0\\) with nearly no ice corresponding to GI=\\(0\\), and one maximum state \\({\\rm CL}_1\\) in terms of glacier extent corresponding to GI=\\(1\\). The climate \\({\\rm CL}\\) consists of a set of variables: mean temperature, temperature variability, mean precipitation, and lapse rate:</p> \\[{\\rm CL}(t) =(T^{\\rm mean}(t),T^{\\rm std}(t),P^{\\rm mean}(t), {\\rm LR}(t)),\\] <p>and is assumed to be a linear combination:</p> \\[{\\rm CL}(t) = {\\rm GI}(t) \\times {\\rm CL}_{\\rm 1} (t) + (1-{\\rm GI}(t)) \\times {\\rm CL}_{\\rm 0} (t),\\] <p>where the two climate states are:</p> \\[\\begin{align} {\\rm CL}_0(t) &amp; = ( T_0^{\\rm mean}(t),T_0^{\\rm std}(t),P_0^{\\rm mean}(t), {\\rm LR}_0(t)), \\\\ {\\rm CL}_1(t) &amp; = ( T_1^{\\rm mean}(t),T_1^{\\rm std}(t),P_1^{\\rm mean}(t), {\\rm LR}_1(t)), \\end{align}\\] <p>corresponding to GI=\\(0\\) and GI=\\(1\\), respectively. Lastly, the GI function is built by linearly rescaling a climate proxy signal so that GI is close to 1 at the ice maximum and close to 0 at the ice minimum. For example, we may use the Antarctica EPICA temperature anomaly signal, which is available for the last 800,000 years.</p> <p>Note that the two climates \\({\\rm CL}_0\\) and \\({\\rm CL}_1\\) are defined on two different reference topographies. Therefore, it is necessary to correct the temperature for a shift in elevation using a vertical lapse rate between the modeled ice surface elevation and the reference surface.</p> <p>Contributors: G. Jouvet</p> <p>Reference: [1] Jouvet, Guillaume, et al. \"Coupled climate-glacier modelling of the last glaciation in the Alps.\" Journal of Glaciology 69.278 (2023): 1956-1970.</p>"},{"location":"modules/processes/clim_glacialindex/#config-structure","title":"Config Structure","text":"<pre><code>clim_glacialindex:\n  update_freq: 100\n  climate_0_file: data/climate.nc\n  climate_1_file: data/climate1.nc\n  signal_file: data/GI.dat\n  vertical_lapse_rate_0: 6.0\n  vertical_lapse_rate_1: 5.74\n  temporal_resampling: 12\n</code></pre>"},{"location":"modules/processes/clim_glacialindex/#parameters","title":"Parameters","text":"Name Type Units Description Default Value update_freq float y Frequency at which to update the climate module 100 climate_0_file str dimless Climate file for glacial index 0 data/climate.nc climate_1_file str dimless Climate file for glacial index 1 data/climate1.nc signal_file str dimless Climate signal file for GI data/GI.dat vertical_lapse_rate_0 float ^\\circC km^{-1} Vertical lapse rate for glacier index 0 6.0 vertical_lapse_rate_1 float ^\\circC km^{-1} Vertical lapse rate for glacier index 1 5.74 temporal_resampling int y^{-1} Temporal resampling, number of time steps per year 12 <p>--&gt;</p>"},{"location":"modules/processes/clim_oggm/","title":"Module <code>clim_oggm</code>","text":"<p>The <code>clim_oggm</code> module processes monthly historical climate data from the GSWP3_W5E5 dataset, obtained via the <code>oggm_shop</code> module. It generates monthly 2D raster fields for corrected precipitation, mean temperature, and temperature variability. The module applies a multiplicative correction factor to precipitation (<code>prcp_fac</code>) and a bias correction to temperature (<code>temp_bias</code>). Temperature data is extrapolated across the glacier surface using a reference height and a constant lapse rate (<code>temp_default_gradient</code>). In contrast, precipitation and temperature variability data are distributed across the entire domain without additional corrections. All necessary calibrated parameters are provided by the <code>oggm_shop</code> module. These outputs are intended for use in surface mass balance or enthalpy modeling.</p> <p>The module also supports generating climate data beyond the observational time frame. This is achieved by defining a reference period (<code>ref_period</code>) to randomly select years within this interval (typically a climate-neutral period). A temperature bias and a precipitation scaling are then applied. These parameters can be specified in a file (<code>file</code> parameter), as shown in the example below. The example illustrates a linear temperature increase of 4 degrees by 2100 (relative to the 1960\u20131990 period):</p> <pre><code>time   delta_temp  prec_scal\n1900          0.0        1.0\n2020          0.0        1.0\n2100          4.0        1.0\n</code></pre> <p>If the <code>clim_trend_array</code> parameter is set to an empty list (<code>[]</code>), the module reads the data from the specified <code>file</code>. Otherwise, it uses the <code>clim_trend_array</code> parameter, which must be a list of lists.</p> <p>Contributors: Guillaume Jouvet, Fabien Maussion</p>"},{"location":"modules/processes/clim_oggm/#config-structure","title":"Config Structure","text":"<pre><code>clim_oggm:\n  update_freq: 1\n  file: file.txt\n  clim_trend_array:\n    - [\"time\", \"delta_temp\", \"prec_scal\"]\n    - [1900, 0.0, 1.0]\n    - [2020, 0.0, 1.0]\n  ref_period: [1960, 1990]\n  seed_par: 123\n</code></pre>"},{"location":"modules/processes/clim_oggm/#parameters","title":"Parameters","text":"<p>Here we store a table with</p> Name Type Units Description Default Value update_freq float y Update the climate each X years 1 file string dimless Name of the input file for the climate outside the given datetime frame (time, delta_temp, prec_scaling) file.txt clim_trend_array list dimless Define climate trend outside available time window [['time', 'delta_temp', 'prec_scal'], [1900, 0.0, 1.0], [2020, 0.0, 1.0]] ref_period list dimless Define the reference period to pick year outside available time window [1960, 1990] seed_par integer dimless Seeding parameter to fix for picking randomly year in the ref period 123 <p>--&gt;</p>"},{"location":"modules/processes/data_assimilation/","title":"Module <code>data_assimilation</code>","text":"<p>A data assimilation module in IGM allows users to determine the optimal ice thickness, top ice surface, and/or ice flow parameters that best match observational data, such as surface ice velocities, ice thickness profiles, and top ice surface elevation, while maintaining consistency with the ice flow emulator (<code>iceflow</code>) used in forward modeling. This page provides guidance on using the data assimilation module as a preparatory step for running a forward or prognostic model in IGM. Check at the IGM technical paper for further details [1].</p> <p>[1] Concepts and capabilities of the Instructed Glacier Model 3.X.X, Jouvet and al.</p> <p>Note: The optimization process requires some expertise, and parameter tuning may be necessary to achieve meaningful results. Use this module carefully and be prepared to explore various parameter configurations. Feel free to contact us to verify the consistency of your results.</p>"},{"location":"modules/processes/data_assimilation/#getting-the-data","title":"Getting the data","text":"<p>The first step is to gather as much relevant data as possible. The recommended data includes:</p> <ul> <li>Observed surface ice velocities (\\({\\bf u}^{s,obs}\\)), e.g., from Millan et al. (2022).</li> <li>Surface top elevation (\\(s^{obs}\\)), e.g., from datasets like SRTM or ESA GLO-30.</li> <li>Ice thickness profiles (\\(h_p^{obs}\\)), e.g., from the GlaThiDa database.</li> <li>Glacier outlines and resulting mask, e.g., from the Randolph Glacier Inventory (RGI).</li> </ul> <p>If you do not have access to all these datasets, it is still possible to proceed with a reduced dataset. However, in such cases, you will need to make assumptions to limit the number of variables to optimize (controls). This ensures that the optimization problem remains well-posed, meaning it has a unique and meaningful solution.</p> <p>These data can be obtained using the IGM module <code>oggm_shop</code> and loaded with the inputs module using convention-based variable names ending with <code>obs</code>. For example:</p> <ul> <li><code>usurfobs</code>: Observed top surface elevation.</li> <li><code>thkobs</code>: Observed thickness profiles (use <code>NaN</code> or no-value where no data is available).</li> <li><code>icemaskobs</code>: Mask derived from RGI outlines to enforce zero ice thickness outside the mask.</li> <li><code>uvelsurfobs</code> and <code>vvelsurfobs</code>: X- and Y-components of the horizontal surface ice velocity (use <code>NaN</code> or no-value where no data is available).</li> <li><code>thkinit</code>: Optionally, a previously inferred ice thickness field to initialize the inverse model. If not provided, the model will start with <code>thk=0</code>.</li> </ul> <p>Use the IGM <code>oggm_shop</code> to download all the data you need using OGGM.</p>"},{"location":"modules/processes/data_assimilation/#general-optimization-setting","title":"General optimization setting","text":"<p>The optimization problem consists of finding spatially varying fields (\\(h\\), \\(A\\), \\(c\\), \\(s\\)) that minimize the cost function:</p> \\[\\mathcal{J}(h,A,c,s)=\\mathcal{C}^u+\\mathcal{C}^h+\\mathcal{C}^s+\\mathcal{C}^{d}+\\mathcal{R}^h+\\mathcal{R}^A+\\mathcal{R}^{c}+\\mathcal{P}^h,\\] <p>where:</p> <ul> <li>\\(\\mathcal{C}^u\\): Misfit between modeled and observed surface ice velocities.</li> <li>\\(\\mathcal{C}^h\\): Misfit between modeled and observed ice thickness profiles.</li> <li>\\(\\mathcal{C}^s\\): Misfit between modeled and observed top ice surface.</li> <li>\\(\\mathcal{C}^d\\): Misfit term between modeled and observed flux divergence.</li> <li>\\(\\mathcal{R}^h\\): Regularization term to enforce smoothness (and possible convexity) on \\(h\\).</li> <li>\\(\\mathcal{R}^A\\): Regularization term to enforce smoothness (and possible convexity) on \\(A\\).</li> <li>\\(\\mathcal{R}^c\\): Regularization term to enforce smoothness on \\(c\\).</li> <li>\\(\\mathcal{P}^h\\): Penalty term to enforce nonnegative ice thickness.</li> </ul> <p>This formulation ensures that the optimization problem is well-posed by balancing data fidelity terms (\\(\\mathcal{C}\\)) with regularization and penalty terms (\\(\\mathcal{R}\\) and \\(\\mathcal{P}\\)). Check at the reference paper for more explanation on each terms of the cost function.</p>"},{"location":"modules/processes/data_assimilation/#define-controls-and-cost-components","title":"Define controls and cost components","text":"<p>The above optimization problem is given in the most general case. However, you may select only some components according to your data as follows:</p> <ul> <li> <p>Control Variables: Specify the variables you wish to optimize. For example:   <pre><code>\"processes.data_assimilation.control_list\": [\"thk\", \"slidingco\", \"usurf\"]  # Optimize ice thickness, sliding coefficient, and surface elevation.\n\"processes.data_assimilation.control_list\": [\"thk\", \"usurf\"]  # Optimize ice thickness and surface elevation only.\n\"processes.data_assimilation.control_list\": [\"thk\"]  # Optimize ice thickness only.\n</code></pre></p> </li> <li> <p>Cost Components: Specify the components of the cost function to minimize. For example:   <pre><code>\"processes.data_assimilation.cost_list\": [\"velsurf\", \"thk\", \"usurf\", \"divfluxfcz\", \"icemask\"]  # General case with multiple components.\n\"processes.data_assimilation.cost_list\": [\"velsurf\", \"icemask\"]  # Fit surface velocity and ice mask only.\n</code></pre></p> </li> </ul> <p>Recommendation: Start with a simple optimization setup, such as a single control variable (<code>thk</code>) and a few cost components (<code>velsurf</code> and <code>icemask</code>). Gradually increase the complexity by adding more controls and cost components once the simpler setup yields meaningful results. Ensure a balance between controls and constraints to maintain a well-posed problem and avoid multiple solutions.</p>"},{"location":"modules/processes/data_assimilation/#exploring-parameters","title":"Exploring parameters","text":"<p>There are parameters that may need to tune for each application.</p> <p>First, you may adjust the expected confidence levels (i.e., tolerance to fit the data) \\(\\sigma^u\\), \\(\\sigma^h\\), \\(\\sigma^s\\), and \\(\\sigma^d\\) to better match surface ice velocity, ice thickness, surface top elevation, or flux divergence. These parameters can be configured as follows:</p> <pre><code>\"processes.data_assimilation.velsurfobs_std\": 5 # unit m/y\n\"processes.data_assimilation.thkobs_std\" : 5 # unit m\n\"processes.data_assimilation.usurfobs_std\" : 5 # unit m\n\"processes.data_assimilation.divfluxobs_std\": 1 # unit m/y\n</code></pre> <p>Second, you may adjust regularization parameters to control the smoothness and convexity of the optimized fields. These include:</p> <ol> <li> <p>Regularization Weights (\\(\\alpha^h\\), \\(\\alpha^A\\)): These parameters control the regularization strength for ice thickness and flow parameters. Increasing \\(\\alpha^h\\) or \\(\\alpha^A\\) will result in smoother spatial fields for these variables.</p> </li> <li> <p>Convexity weight (\\(\\gamma\\)): Adds a convexity constraint to the system. Using a small value for \\(\\gamma\\) can help when initializing the inverse model with zero ice thickness or when dealing with margin regions lacking observational data.</p> </li> </ol> <p>These parameters can be configured as follows:</p> <pre><code>\"processes.data_assimilation.regu_param_thk\": 10.0           # Regularization weight for ice thickness\n\"processes.data_assimilation.regu_param_slidingco\": 1.0      # Regularization weight for sliding coefficient\n\"processes.data_assimilation.convexity_weight\": 0.002        # Convexity weight (gamma)\n</code></pre> <p>Lastly, there are a couple of other parameters we may be interest to change e.g.</p> <pre><code>\"processes.data_assimilation.nbitmax\": 1000         # Number of it. for the optimization\n\"processes.data_assimilation.step_size\": 1.0        # Step size in the optimization iterative algorithm\n\"processes.data_assimilation.init_zero_thk\": True   # Force init zero ice thk (otherwise take thkinit)\n</code></pre>"},{"location":"modules/processes/data_assimilation/#parameter-inference-s-cook","title":"Parameter inference (S. Cook)","text":"<p>There is also a further option: the convexity weight and the slidingco can be inferred automatically by the model. These values are calibrated only for IGM v2.2.1 and a particular set of costs and controls, and are based on a series of regressions calculated through manual inversions to find the best parameters for 50 glaciers of different types and sizes around the world (see Samuel's forthcoming paper when it's published). In other words, they are purely empirical and are likely to be a bit off for any different set of costs and controls, but should work tolerably well on any glacier anywhere on the planet, at least to give you somewhere to start exploring the parameter space. If this behaviour is desired, you MUST use RGI7.0 (C or G) and the oggm_shop module. If using C, you will also need to set the oggm_sub_entity_mask parameter to True. Within the optimize module, processes.data_assimilation.infer_params must also be set to true.</p> <p>For small glaciers with no velocity observations, the model will also use volume-area scaling to provide an additional constraint with in the inference framework - this all happens automatically, but note the processes.data_assimilation.vol_std parameter that you can fiddle around with if you want to force it to pay more or less attention to volume (by default, this is 1000.0 - which will give a very small cost - anywhere with velocity data, and 0.001 - which will give a big cost - anywhere lacking velocity data. The parameter only controls the default value where this is other data - the 0.001 where there's no velocity data is hard-coded).</p> <p>A final parameter - processes.data_assimilation.tidewater_glacier - can also be set to True to force the inference code to treat the glacier as a tidewater-type glacier. If the RGI identifies a glacier as tidewater, it will be treated as such anyway, but this parameter gives you the option to force it (note: setting the parameter to False - its default value - will not cause the model to treat RGI-identified tidewater glaciers as non-tidewater - there is no option to do that).</p>"},{"location":"modules/processes/data_assimilation/#monitoring-the-optimization","title":"Monitoring the Optimization","text":"<p>You can monitor the data assimilation process during inverse modeling in several ways:</p> <ul> <li>Cost Components: Verify that the components of the cost function decrease over time. The cost values are printed during the optimization process, and a graph summarizing the results is generated at the end.</li> <li>Live Monitoring: Set the parameters <code>\"plot_result\": true</code> and <code>\"plt2d_live\": true</code> to visualize the evolution of the optimized fields (e.g., ice thickness, surface ice speeds) in real-time. Additionally, observe the (hopefully decreasing) standard deviations displayed in the figures.</li> <li>Post-Run Analysis: After the run, examine the <code>optimize.nc</code> file, which contains the results of the optimization. Ensure this file is configured to be written during the process.</li> <li>Flux Divergence Check: If <code>divfluxfcz</code> is included in the parameter list <code>\"processes.data_assimilation.cost\"</code>, inspect the divergence of the flux to ensure it aligns with expectations.</li> </ul> <p>For more information, refer to the relevant documentation or technical references.</p> <p>[2] Jouvet, Guillaume. \"Inversion of a Stokes glacier flow model emulated by deep learning.\" Journal of Glaciology 69.273 (2023): 13-26.</p> <p>[3] Jouvet, Guillaume, and Guillaume Cordonnier. \"Ice-flow model emulator based on physics-informed deep learning.\" Journal of Glaciology 69.278 (2023): 1941-1955.</p> <p>Contributors: G. Jouvet, S. Cook (parameter inference functions for global modelling)</p>"},{"location":"modules/processes/data_assimilation/#config-structure","title":"Config Structure","text":"<pre><code>data_assimilation:\n  control_list:\n  - thk\n  cost_list:\n  - velsurf\n  - thk\n  - icemask\n  optimization:\n    nbitmin: 50\n    nbitmax: 500\n    step_size: 1.0\n    step_size_decay: 0.9\n    init_zero_thk: false\n    sole_mask: false\n    retrain_iceflow_model: true\n    fix_opti_normalization_issue: false\n    method: ADAM\n    obstacle_constraint: [reproject,penalty]\n    optimizer_clipnorm: 10000000.0\n    optimizer_epsilon: 1.0e-07\n    pertubate: False\n  regularization:\n    thk: 10.0\n    slidingco: 1.0\n    arrhenius: 10.0\n    divflux: 1.0\n    smooth_anisotropy_factor: 0.2\n    smooth_anisotropy_factor_sl: 1.0\n    convexity_weight: 0.002\n    convexity_power: 1.3\n    to_regularize: topg\n  fitting:\n    usurfobs_std: 2.0\n    velsurfobs_std: 1.0\n    thkobs_std: 3.0\n    divfluxobs_std: 1.0\n    uniformize_thkobs: true\n    include_low_speed_term: false\n    velsurfobs_thr: 0.0\n    log_slidingco: false\n  divflux:\n    method: upwind\n    force_zero_sum: false\n  scaling:\n    thk: 2.0\n    usurf: 0.5\n    slidingco: 0.0001\n    arrhenius: 0.1\n  output:\n    freq: 50\n    plot2d_live: true\n    plot2d: true\n    save_result_in_ncdf: geology-optimized.nc\n    save_iterat_in_ncdf: true\n    editor_plot2d: vs\n    vars_to_save:\n    - usurf\n    - thk\n    - slidingco\n    - velsurf_mag\n    - velsurfobs_mag\n    - divflux\n    - icemask\n  cook:\n    infer_params: false\n    tidewater_glacier: false\n    vol_std: 1000.0\n</code></pre>"},{"location":"modules/processes/data_assimilation/#parameters","title":"Parameters","text":"Name Type Units Description Default Value control_list list \\( dimless \\) List of optimized variables for the optimization ['thk'] cost_list list \\( dimless \\) List of cost components for the optimization ['velsurf', 'thk', 'icemask'] optimization.nbitmin integer \\( dimless \\) Min iterations for the optimization 50 optimization.nbitmax integer \\( dimless \\) Max iterations for the optimization 500 optimization.step_size float \\( dimless \\) Step size for the optimization 1.0 optimization.step_size_decay float \\( dimless \\) Decay step size parameter for the optimization 0.9 optimization.init_zero_thk boolean \\( dimless \\) Initialize the optimization with zero ice thickness False optimization.sole_mask boolean \\( dimless \\) sole_mask False optimization.retrain_iceflow_model boolean \\( dimless \\) Retrain the iceflow model simultaneously? True optimization.fix_opti_normalization_issue boolean \\( dimless \\) Formerly, the oce was mixing reduce_mean and l2_loss leading to dependence to the resolution of the grid False optimization.method string \\( dimless \\) Optimization method to use, either ADAM or L-BFGS ADAM optimization.obstacle_constraint list \\( dimless \\) Actions to apply obstacle constraints, either reproject or penalty or both (current default) ['reproject', 'penalty'] optimization.optimizer_clipnorm float \\( dimless \\) If set, the gradient of each weight is individually clipped so that its norm is no higher than this value. 10000000.0 optimization.optimizer_epsilon float \\( dimless \\) A small constant for numerical stability for the Adam optimizer 1e-07 optimization.pertubate boolean \\( dimless \\) Perturbate the emulator for training to get better gradients estimates along controls to be optimized False regularization.thk float \\( dimless \\) Regularization weight for the ice thickness in the optimization 10.0 regularization.slidingco float \\( dimless \\) Regularization weight for the slidingco field in the optimization 1.0 regularization.arrhenius float \\( dimless \\) Regularization weight for the arrhenius field in the optimization 10.0 regularization.divflux float \\( dimless \\) Regularization weight for the divergence field in the optimization 1.0 regularization.smooth_anisotropy_factor float \\( dimless \\) Smooth anisotropy factor for the ice thickness regularization in the optimization 0.2 regularization.smooth_anisotropy_factor_sl float \\( dimless \\) Smooth anisotropy factor for the slidingco regularization in the optimization 1.0 regularization.convexity_weight float \\( dimless \\) Convexity weight for the ice thickness regularization in the optimization 0.002 regularization.convexity_power float \\( dimless \\) Power b in the area-volume scaling V ~ a * A^b taken from 'An estimate of global glacier volume', A. Grinsted, TC, 2013 1.3 regularization.to_regularize string \\( dimless \\) Field to regularize: topg or thk topg fitting.usurfobs_std float \\( m \\) Confidence/STD of the top ice surface as input data for the optimization 2.0 fitting.velsurfobs_std float \\( m y^{-1} \\) Confidence/STD of the surface ice velocities as input data for the optimization (if 0, velsurfobs_std field must be given) 1.0 fitting.thkobs_std float \\( m \\) Confidence/STD of the ice thickness profiles (unless given) 3.0 fitting.divfluxobs_std float \\( ??? \\) Confidence/STD of the flux divergence as input data for the optimization (if 0, divfluxobs_std field must be given) 1.0 fitting.uniformize_thkobs boolean \\( dimless \\) uniformize the density of thkobs True fitting.include_low_speed_term boolean \\( dimless \\) opti_include_low_speed_term False fitting.velsurfobs_thr float \\( m y^{-1} \\) Threshold for the surface ice velocities as input data for the optimization, anything below this value will be ignored 0.0 fitting.log_slidingco boolean \\( dimless \\) Optimize the log of the sliding coefficient instead of the slidingco itself False divflux.method string \\( dimless \\) Compute the divergence of the flux using the upwind or centered method upwind divflux.force_zero_sum boolean \\( dimless \\) Add a penalty to the cost function to force the sum of the divergence of the flux to be zero False scaling.thk float \\( dimless \\) Scaling factor for the ice thickness in the optimization, serves to adjust step-size of each control relative to each other 2.0 scaling.usurf float \\( dimless \\) Scaling factor for the ice thickness in the optimization, serves to adjust step-size of each control relative to each other 0.5 scaling.slidingco float \\( dimless \\) Scaling factor for the slidingco in the optimization, serves to adjust step-size of each control relative to each other 0.0001 scaling.arrhenius float \\( dimless \\) Scaling factor for the Arrhenius in the optimization, serves to adjust step-size of each control relative to each other 0.1 output.freq integer \\( dimless \\) Frequency of the output for the optimization 50 output.plot2d_live boolean \\( dimless \\) plot2d_live_inversion True output.plot2d boolean \\( dimless \\) plot 2d inversion True output.save_result_in_ncdf file path \\( dimless \\) Geology input file geology-optimized.nc output.save_iterat_in_ncdf boolean \\( dimless \\) Sve the iterations in a ncdf file True output.editor_plot2d string \\( dimless \\) optimized for VS code (vs) or spyder (sp) for live plot vs output.vars_to_save list \\( dimless \\) List of variables to be recorded in the ncdf file ['usurf', 'thk', 'slidingco', 'velsurf_mag', 'velsurfobs_mag', 'divflux', 'icemask'] cook.infer_params boolean \\( dimless \\) Infer slidingco and convexity weight from velocity observations False cook.tidewater_glacier boolean \\( dimless \\) Is the glacier you're trying to infer parameters for a tidewater type? False cook.vol_std float \\( ??? \\) Confidence/STD of the volume estimates from volume-area scaling 1000.0"},{"location":"modules/processes/enthalpy/","title":"Module <code>enthalpy</code>","text":"<p>Warning: this rather complex module was not much tested so far, use it with care!</p>"},{"location":"modules/processes/enthalpy/#description","title":"Description:","text":"<p>This IGM module models the ice enthalpy, which permits to jointly model the ice temperature, as well as the water content created when the temperature hits the pressure melting points, and therefore energy conservation, which is not the case when modelling the sole temperature variable. The model is described in (Aschwanden and al, JOG, 2012). Check at the IGM technical paper for further details [1].</p> <p>[1] Concepts and capabilities of the Instructed Glacier Model 3.X.X, Jouvet and al.</p> <p>The enthalpy module builds upon the <code>iceflow</code> module. To ensure proper functionality, follow these requirements:</p> <ul> <li>Activate the <code>vertical_iceflow</code> module to provide the vertical velocity.</li> <li>Set <code>params.dim_arrhenius = 3</code>.</li> <li>Set <code>params.new_friction_param = true</code>.</li> <li>Ensure sufficient retraining by setting <code>retrain_iceflow_emulator_freq = 1</code>. Optionally, set <code>retrain_iceflow_emulator_nbit</code> to a value greater than 1 for improved performance.</li> </ul> <p>Contributors: G. Jouvet</p> <p>This implementation is largely inspired from the one implemented in PISM. Other references that have helped are (Kleiner and al, TC, 2015) and (Wang and al, 2020).</p>"},{"location":"modules/processes/enthalpy/#config-structure","title":"Config Structure","text":"<pre><code>enthalpy:  \n  water_density: 1000.0\n  spy: 31556926.0\n  ki: 2.1\n  ci: 2009.0\n  Lh: 3.34e+5\n  KtdivKc: 1e-1\n  claus_clape: 7.9e-8\n  melt_temp: 273.15\n  ref_temp: 223.15\n  till_friction_angle: 30.0\n  till_friction_angle_bed_min: null\n  till_friction_angle_bed_max: null\n  till_friction_angle_phi_min: 15.0\n  till_friction_angle_phi_max: 45.0\n  uthreshold: 100.0\n  drain_rate: 0.001\n  till_wat_max: 2.0\n  drain_ice_column: True\n  default_bheatflx: 0.065\n  temperature_offset_air_to_ice: 0.0\n  tauc_min: 1.0e+5\n  tauc_max: 1.0e+10\n</code></pre>"},{"location":"modules/processes/enthalpy/#parameters","title":"Parameters","text":"Name Type Units Description Default Value water_density float \\( kg~m^{-3} \\) Constant of the Water density 1000.0 spy float \\( s y^{-1} \\) Number of seconds per years 31556926.0 ki float \\( W~m^{-1}~K^{-1} \\) Conductivity of cold ice (Aschwanden and al, JOG, 2012) 2.1 ci float \\( W~s~kg^{-1}~K^{-1} \\) Specific heat capacity of ice (Aschwanden and al, JOG, 2012) 2009.0 Lh float \\( W~s~kg^{-1} = E \\) Latent heat of fusion (Aschwanden and al, JOG, 2012) 334000.0 KtdivKc float \\( \\frac{Kt}{Kc} \\) Ratio of temp vs cold ice diffusivity (Aschwanden and al, JOG, 2012) 1e-1 claus_clape float \\( K~Pa^{-1} \\) Clausius-Clapeyron constant (Aschwanden and al, JOG, 2012) 7.9e-08 melt_temp float \\( K \\) Melting point at standard pressure (Aschwanden and al, JOG, 2012) 273.15 ref_temp float \\( K \\) Reference temperature (Aschwanden and al, JOG, 2012) 223.15 till_friction_angle float \\( \\circ \\) Till friction angle in the Mohr-Coulomb friction law 30.0 till_friction_angle_bed_min float \\( \\circ \\) Minimum till friction angle at the bed None till_friction_angle_bed_max float \\( \\circ \\) Maximum till friction angle at the bed None till_friction_angle_phi_min float \\( \\circ \\) Minimum till friction angle phi 15.0 till_friction_angle_phi_max float \\( \\circ \\) Maximum till friction angle phi 45.0 uthreshold float \\( \\frac{m}{y} \\) Threshold velocity 100.0 drain_rate float \\( \\frac{m}{y} \\) Drain rate at 1 mm/y (Bueler and Pelt, GMD, 2015) 0.001 till_wat_max float \\( m \\) Maximum water till thickness (Bueler and Pelt, GMD, 2015) 2.0 drain_ice_column float \\( m \\) Transform the water content beyond a threshold into water, drain it, and add it to basal melt rate True default_bheatflx float \\( W~m^{-2} \\) Geothermal heat flux 0.065 temperature_offset_air_to_ice float \\( K \\) Offset between the air temperature and the ice temperature 0.0 tauc_min float \\( Pa \\) Lower bound for tauc 100000.0 tauc_max float \\( Pa \\) Upper bound for tauc 10000000000.0"},{"location":"modules/processes/gflex/","title":"Module <code>gflex</code>","text":"<p>This IGM module models isostasy or the upward motion of the lithosphere when loaded with thick ice. It utilizes the gflex Python module developed by Andy Wickert.</p> <p>The key parameters are the update frequency <code>processes.gflex.update_freq</code> and the Elastic Thickness (Te) in meters, specified as <code>processes.gflex.default_Te</code>.</p> <p>This module operates exclusively on the CPU, which may pose challenges when processing very large arrays. However, since updates are not expected to occur frequently, the overall computational demand of this module should remain manageable.</p> <p>Contributors: J\u00fcrgen Mey</p>"},{"location":"modules/processes/gflex/#config-structure","title":"Config Structure","text":"<pre><code>gflex:\n  update_freq: 100.0\n  default_Te: 50000\n  dx: 1000\n  pad: False\n  quiet: True\n</code></pre>"},{"location":"modules/processes/gflex/#parameters","title":"Parameters","text":"Name Type Units Description Default Value update_freq float y Update gflex each X years 100.0 default_Te float m Default value for Te (Elastic thickness) if not given as ncdf file 50000 dx float m Default resolution for computing isostasy 1000 pad boolean dimless Default padding option False quiet boolean dimless ??? True"},{"location":"modules/processes/glerosion/","title":"glerosion","text":"<p>Module <code>glerosion</code></p> <p>This IGM module implements change in basal topography (due to glacial erosion). The bedrock is updated (with a frequency provided by parameter <code>processes.glerosion.update_freq years</code>) assuming a power erosion law, i.e. the erosion rate is proportional (parameter <code>processes.glerosion.cst</code>) to a power (parameter <code>processes.glerosion.exp</code>) of the sliding velocity magnitude. By default, we use the parameters from [1].</p> <p>[1] Herman, F. et al., Erosion by an Alpine glacier. Science 350, 193-195, 2015.</p> <p>Contributors: G. Jouvet</p>"},{"location":"modules/processes/glerosion/#config-structure","title":"Config Structure","text":"<pre><code>glerosion:\n  cst: 2.7e-7\n  exp: 2\n  update_freq: 1\n</code></pre>"},{"location":"modules/processes/glerosion/#parameters","title":"Parameters","text":"Name Type Units Description Default Value cst float dimless Erosion multiplicative factor, here taken from Herman, F. et al. Erosion by an Alpine glacier. Science 350, 193\u2013195 (2015) 2.7e-07 exp float dimless Erosion exponent factor, here taken from Herman, F. et al. Eerosion by an Alpine glacier. Science 350, 193\u2013195 (2015) 2 update_freq float y Update the erosion only each X years 1"},{"location":"modules/processes/iceflow/","title":"Module <code>iceflow</code>","text":"<p>This IGM module models ice flow dynamics in 3D using a Convolutional Neural Network (CNN) based on a Physics-Informed Neural Network (PINN), as described in [1]. Specifically, the CNN is trained to minimize the energy associated with high-order ice flow equations during the time iterations of a glacier evolution model. Consequently, it serves as a computationally efficient alternative to traditional solvers, capable of handling diverse ice flow regimes. Check at the IGM technical paper for further details [1].</p> <p>[1] Concepts and capabilities of the Instructed Glacier Model 3.X.X, Jouvet and al.</p> <p>Pre-trained emulators are provided by default. However, one may start from scratch by setting <code>processes.iceflow.emulator.name=\"\"</code>. The key parameters to consider in this case are:</p> <ul> <li>Physical parameters:</li> </ul> <pre><code>\"processes.iceflow.physics.init_slidingco\": 0.045    # Init slid. coeff. ($Mpa y^{1/3} m^{-1/3}$)\n\"processes.iceflow.physics.init_arrhenius\": 78.0     # Init Arrhenius cts ($Mpa^{-3} y^{-1}$)\n\"processes.iceflow.physics.exp_glen\": 3              # Glen's exponent\n\"processes.iceflow.physics.exp_weertman\":  3         # Weertman's sliding law exponent\n</code></pre> <ul> <li>Numerical parameters for the vertical discretization:</li> </ul> <pre><code>\"processes.iceflow.numerics.Nz\": 10                 # number of vertical layers\n\"processes.iceflow.numerics.vert_spacing\": 4.0      # 1.0 for equal vertical spacing, 4.0 otherwise\n</code></pre> <p>Note that in the special case of \\(Nz=2\\), the ice velocity profile from the bottom to the top of the ice is assumed to vary polynomially following the Shallow Ice Approximation (SIA) formula. In the case of a single layer \\(Nz=1\\), the ice flow is assumed to be vertically uniform, and the ice flow model reduces to the Shallow Shelf Approximation (SSA).</p> <ul> <li>Learning rate and frequency of retraining:</li> </ul> <pre><code>\"processes.iceflow.emulator.lr\": 0.00002 \n\"processes.iceflow.emulator.retrain_freq\": 5     \n</code></pre> <p>While this module was targeted for deep learning emulation, it is possible to use the solver (<code>processes.iceflow.method='solved'</code>) instead of the default (<code>processes.iceflow.method='emulated'</code>), or use the two together (<code>processes.iceflow.method='diagnostic'</code>) to assess the emulator against the solver. The most important parameters for solving are:</p> <pre><code>\"processes.iceflow.solve.step_size\": 0.00002 \n\"processes.iceflow.solve.nbitmax\": 5         \n</code></pre> <p>One may choose between a 2D Arrhenius factor or a 3D Arrhenius factor by setting the parameter <code>processes.iceflow.dim_arrhenius</code> to <code>2</code> or <code>3</code>, respectively. The 3D option is required for the enthalpy model.</p> <p>When treating very large arrays, retraining must be done sequentially in a patch-wise manner due to memory constraints. The size of the patch is controlled by the parameter <code>processes.iceflow.multiple_window_size=750</code>.</p> <p>Contributors: G. Jouvet</p>"},{"location":"modules/processes/iceflow/#config-structure","title":"Config Structure","text":"<pre><code>iceflow:\n  method: emulated\n  force_max_velbar: 0.0\n  physics:\n    gravity_cst: 9.81\n    ice_density: 910.0\n    init_slidingco: 0.0464\n    init_arrhenius: 78.0\n    enhancement_factor: 1.0\n    exp_glen: 3.0\n    exp_weertman: 3.0\n    regu_glen: 1.0e-05\n    regu_weertman: 1.0e-10\n    new_friction_param: true\n    dim_arrhenius: 2\n    regu: 0.0\n    thr_ice_thk: 0.1\n    min_sr: 1.0e-20\n    max_sr: 1.0e+20\n    force_negative_gravitational_energy: false\n    cf_eswn: []\n    cf_cond: false\n  numerics:\n    Nz: 10\n    vert_spacing: 4.0\n  solver:\n    step_size: 1.0\n    nbitmax: 100\n    stop_if_no_decrease: true\n    optimizer: Adam\n    lbfgs: false\n    save_cost: ''\n    plot_sol: False\n  emulator:\n    fieldin:\n    - thk\n    - usurf\n    - arrhenius\n    - slidingco\n    - dX\n    retrain_freq: 10\n    lr: 2.0e-05\n    lr_init: 0.0001\n    lr_decay: 0.95\n    warm_up_it: -10000000000.0\n    nbit_init: 1\n    nbit: 1\n    framesizemax: 750\n    pretrained: true\n    name: ''\n    save_model: false\n    exclude_borders: 0\n    optimizer: Adam\n    optimizer_clipnorm: 1.0\n    optimizer_epsilon: 1.0e-07\n    save_cost: ''\n    output_directory: ''\n    plot_sol: False\n    network:\n      architecture: cnn\n      multiple_window_size: 0\n      activation: LeakyReLU\n      nb_layers: 16\n      nb_blocks: 4\n      nb_out_filter: 32\n      conv_ker_size: 3\n      dropout_rate: 0\n      weight_initialization: glorot_uniform\n      cnn3d_for_vertical: false\n</code></pre>"},{"location":"modules/processes/iceflow/#parameters","title":"Parameters","text":"Name Type Units Description Default Value method string \\( dimless \\) Type of iceflow: it can be emulated (default), solved, or in diagnostic mode emulated force_max_velbar float \\( m y^{-1} \\) This permits to artificially upper-bound velocities, active if &gt; 0 0.0 physics.gravity_cst float \\( m^2 s^{-1} \\) Gravitational constant 9.81 physics.ice_density float \\( kg m^{-3} \\) Density of ice 910.0 physics.init_slidingco float \\( Mpa y^m m^{-m} \\) Initial sliding coefficient slidingco 0.0464 physics.init_arrhenius float \\( Mpa^{-n} y^{-1} \\) Initial arrhenius factor arrhenius 78.0 physics.enhancement_factor float \\( dimless \\) Enhancement factor multiplying the arrhenius factor 1.0 physics.exp_glen float \\( dimless \\) Glen's flow law exponent 3.0 physics.exp_weertman float \\( dimless \\) Weertman's law exponent 3.0 physics.regu_glen float \\( dimless \\) Regularization parameter for Glen's flow law 1e-05 physics.regu_weertman float \\( dimless \\) Regularization parameter for Weertman's sliding law 1e-10 physics.new_friction_param boolean \\( dimless \\) Sliding coefficient (this describes slidingco differently with slidingco**-(1.0 / exp_weertman) instead of slidingco as before) True physics.dim_arrhenius integer \\( dimless \\) Dimension of the arrhenius factor (horizontal 2D or 3D) 2 physics.regu float \\( dimless \\) This regularizes the energy forcing ice flow to be smooth in the horizontal direction 0.0 physics.thr_ice_thk float \\( m \\) Threshold Ice thickness for computing strain rate 0.1 physics.min_sr float \\( y^{-1} \\) Minimum strain rate 1e-20 physics.max_sr float \\( y^{-1} \\) Maximum strain rate 1e+20 physics.force_negative_gravitational_energy boolean \\( dimless \\) Force energy gravitational term to be negative False physics.cf_eswn list \\( dimless \\) This forces calving front at the border of the domain in the side given in the list [] physics.cf_cond boolean \\( dimless \\) This forces calving front at the border of the domain in the side given in the list False numerics.Nz integer \\( dimless \\) Number of grid points for the vertical discretization 10 numerics.vert_spacing float \\( dimless \\) Parameter controlling the discretization density to get more points near the bed than near the surface. 1.0 means equal vertical spacing. 4.0 solver.step_size float \\( dimless \\) Step size for the optimizer used when solving Blatter-Pattyn in solver mode 1.0 solver.nbitmax integer \\( dimless \\) Maximum number of iterations for the optimizer used when solving Blatter-Pattyn in solver mode 100 solver.stop_if_no_decrease boolean \\( dimless \\) This permits to stop the solver if the energy does not decrease True solver.optimizer string \\( dimless \\) Type of Optimizer for the solver Adam solver.lbfgs boolean \\( dimless \\) Select the L-BFGS optimizer instead of the Adam optimizer False solver.save_cost string \\( dimless \\) solver.plot_sol False emulator.fieldin list \\( dimless \\) Input fields of the iceflow emulator ['thk', 'usurf', 'arrhenius', 'slidingco', 'dX'] emulator.retrain_freq integer \\( dimless \\) Frequency at which the emulator is retrained, 0 means never, 1 means at each time step, 2 means every two time steps, etc. 10 emulator.lr float \\( dimless \\) Learning rate for the retraining of the emulator 2e-05 emulator.lr_init None \\( dimless \\) Initial Learning rate for the retraining of the emulator 0.0001 emulator.lr_decay float \\( dimless \\) Decay learning rate parameter for the training 0.95 emulator.warm_up_it year \\( dimless \\) Warm-up nb of iteration allowing intense initial training -10000000000.0 emulator.nbit_init integer \\( dimless \\) Number of iterations done at the first time step for the retraining of the emulator 1 emulator.nbit integer \\( dimless \\) Number of iterations done at each time step for the retraining of the emulator 1 emulator.framesizemax 750 emulator.pretrained boolean \\( dimless \\) Do we take a pretrained emulator or start from scratch? True emulator.name string \\( dimless \\) Directory path of the deep-learning pretrained ice flow model, take from the library if empty string emulator.save_model boolean \\( dimless \\) Save the iceflow emulator at the end of the simulation False emulator.exclude_borders integer \\( dimless \\) This is a quick fix of the border issue, otherwise the physics-informed emulator shows zero velocity at the border 0 emulator.optimizer string \\( dimless \\) Type of Optimizer for the emulator Adam emulator.optimizer_clipnorm float \\( dimless \\) If set, the gradient of each weight is individually clipped so that its norm is no higher than this value. 1.0 emulator.optimizer_epsilon float \\( dimless \\) A small constant for numerical stability for the Adam optimizer 1e-07 emulator.save_cost string \\( dimless \\) emulator.output_directory string \\( dimless \\) emulator.plot_sol boolean \\( dimless \\) Perits to plot the solution of the emulator at each time step False emulator.network.architecture string \\( dimless \\) This is the type of network, it can be cnn or unet cnn emulator.network.multiple_window_size integer \\( dimless \\) If a U-net, this forces window size to be a multiple of 2**N 0 emulator.network.activation string \\( dimless \\) Activation function, it can be lrelu, relu, tanh, sigmoid, etc. LeakyReLU emulator.network.nb_layers integer \\( dimless \\) Number of layers in the CNN 16 emulator.network.nb_blocks integer \\( dimless \\) Number of block layers in the U-net 4 emulator.network.nb_out_filter integer \\( dimless \\) Number of output filters in the CNN 32 emulator.network.conv_ker_size integer \\( dimless \\) Size of the convolution kernel 3 emulator.network.dropout_rate float \\( dimless \\) Dropout rate in the CNN 0 emulator.network.weight_initialization string \\( dimless \\) glorot_uniform, he_normal, lecun_normal glorot_uniform emulator.network.cnn3d_for_vertical False"},{"location":"modules/processes/particles/","title":"Module <code>particles</code>","text":"<p>This IGM module implements a particle tracking routine, which computes the trajectories of virtual particles advected by the ice flow. The routine operates in real-time during the forward model run, and a large number of particles can be processed efficiently thanks to the parallel implementation with TensorFlow. The routine includes particle seeding (by default in the accumulation area at regular intervals, though this can be customized) and tracking (advection by the velocity field in 3D). Note that there is currently no strategy for removing particles, which may lead to memory overload when using this routine for long durations and/or with high seeding intensity.</p> <p>There are currently two implementations (selectable via the <code>tracking_method</code> parameter):</p> <ul> <li><code>'simple'</code>: Horizontal and vertical directions are treated differently:</li> <li>In the horizontal plane, particles are advected using the horizontal velocity field (interpolated bi-linearly).</li> <li> <p>In the vertical direction, particles are tracked along the ice column, scaled between 0 (at the bed) and 1 (at the surface), based on their relative position. Particles are always initialized at a relative height of 1 (assumed to be on the surface). The evolution of the particle's position within the ice column over time is computed based on the surface mass balance: the particle deepens when the surface mass balance is positive (the relative height decreases) and re-emerges when the surface mass balance is negative (the relative height increases).</p> </li> <li> <p><code>'3d'</code>: Requires activation of the <code>vert_flow</code> module, which computes the vertical velocity by integrating the divergence of the horizontal velocity. This enables full 3D particle tracking.</p> </li> </ul> <p>Currently, the default <code>tracking_method</code> is set to <code>'simple'</code>, as the <code>'3d'</code> method (and its dependency on <code>vert_flow</code>) requires further testing.</p> <p>You may adapt the seeding strategy to your needs. The default seeding occurs in the accumulation area, with the seeding frequency controlled by the <code>frequency_seeding</code> parameter and the seeding density by the <code>density_seeding</code> parameter. Alternatively, you can define a custom seeding strategy (e.g., seeding near rock walls or nunataks). To do this, redefine the <code>seeding_particles()</code> function in a <code>particles.py</code> file located in the working directory (refer to the example <code>aletsch-1880-2100</code>). When executed, <code>igm_run</code> will override the original <code>seeding_particles()</code> function with the user-defined one.</p> <p>The module requires horizontal velocities (<code>state.U</code>) and vertical velocities (<code>state.W</code>). The vertical velocities are computed using the <code>vert_flow</code> module when the <code>tracking_method</code> is set to <code>'3d'</code>.</p> <p>Note: In the code, the positions of particles are recorded within vectors corresponding to the number of tracked particles: <code>state.xpos</code>, <code>state.ypos</code>, and <code>state.zpos</code>. The variable <code>state.rhpos</code> provides the relative height within the ice column (1 at the surface, 0 at the bed). At each time step, the weight of surface debris contained in each cell of the 2D horizontal grid is computed and stored in the variable <code>state.weight_particles</code>.</p> <p>This IGM module writes particle time-position data into CSV files, as computed by the <code>particles</code> module. The saving frequency is controlled by the parameter <code>processes.time.save</code>, which is defined in the <code>time</code> module.</p> <p>The module also writes the trajectories followed by particles. The data are stored in a folder named <code>trajectory</code> (created if it does not already exist). Files named <code>traj-TIME.csv</code> report the space-time positions of the particles at time <code>TIME</code> with the following structure:</p> <pre><code>ID,  state.xpos,  state.ypos,  state.zpos, state.rhpos,  state.tpos, state.englt\nX,            X,           X,           X,           X,           X,           X,\nX,            X,           X,           X,           X,           X,           X,\nX,            X,           X,           X,           X,           X,           X,\n</code></pre> <p>providing, in turn, the particle ID, x, y, z positions, the relative height within the ice column, the seeding time, and the englacial residence time.</p> <p>Contributors: Guillaume Jouvet, Claire-Mathile St\u00fccki</p>"},{"location":"modules/processes/particles/#config-structure","title":"Config Structure","text":"<pre><code>particles:\n  tracking_method: 3d\n  frequency_seeding: 50\n  density_seeding: 0.2\n  tlast_seeding_init: -1.0e+5000\n  write_trajectories: True\n  add_topography: True\n</code></pre>"},{"location":"modules/processes/particles/#parameters","title":"Parameters","text":"Name Type Units Description Default Value tracking_method str dimless Method for tracking particles (3d or simple) 3d frequency_seeding float y Frequency of seeding 50 density_seeding float dimless Density of seeding (1 means we seed all pixels, 0.2 means we seed each 5 grid cell, etc.) 0.2 tlast_seeding_init float y Initialize the date of last seeding. If default value, the seeding will start the first year of the simulation. Changing this value allows to defer it -inf write_trajectories bool dimless Write the trajectories of the particles True add_topography bool dimless Add topg when writing particles True"},{"location":"modules/processes/pretraining/","title":"Module <code>pretraining</code>","text":"<p>This module performs pretraining of the ice flow <code>iflo_emulator</code> on a glacier catalog to enhance its performance during glacier forward runs. Pretraining can be a computationally intensive task, taking a few hours to complete. This module should be executed independently, without involving any other IGM modules. Below is an example of a parameter file:</p> <pre><code># @package _global_\n\ndefaults:\n  - override /inputs: []\n  - override /processes: [pretraining, iceflow]\n  - override /outputs: []\n\nprocesses:\n  iceflow: \n    Nz : 10\n    dim_arrhenius : 2\n    multiple_window_size : 8\n    nb_layers : 16\n    nb_out_filter : 32\n    network : cnn\n    new_friction_param : True\n    retrain_emulator_lr : 0.0001\n    solve_nbitmax : 1000\n    solve_stop_if_no_decrease : False\n  pretraining:\n    epochs : 1000\n    data_dir: data/surflib3d_shape_100\n    soft_begining: 1000\n    min_slidingco: 0.01\n    max_slidingco: 0.4\n    min_arrhenius: 5\n    max_arrhenius: 400\n</code></pre> <p>To run this module, you first need access to a glacier catalog. A dataset of a glacier catalog (mountain glaciers) commonly used for pretraining IGM emulators is available here: .</p> <p>After downloading (or generating your own dataset), organize the folder <code>surflib3d_shape_100</code> into two subfolders: <code>train</code> and <code>test</code>.</p>"},{"location":"modules/processes/pretraining/#config-structure","title":"Config Structure","text":"<pre><code>pretraining:\n  data_dir: \"surflib3d_shape_100\"\n  batch_size: 1\n  freq_test: 20\n  train_iceflow_emulator_restart_lr: 2500\n  epochs: 5000\n  min_arrhenius: 5.0\n  max_arrhenius: 151.0\n  min_slidingco: 0.0\n  max_slidingco: 20000.0\n  min_coarsen: 0\n  max_coarsen: 2\n  soft_begining: 500\n</code></pre>"},{"location":"modules/processes/pretraining/#parameters","title":"Parameters","text":"Name Type Units Description Default Value data_dir str dimless Directory of the data of the glacier catalog surflib3d_shape_100 batch_size integer dimless Batch size 1 freq_test integer dimless Frequency of the test 20 train_iceflow_emulator_restart_lr integer dimless Restart frequency for the learning rate 2500 epochs integer dimless Number of epochs 5000 min_arrhenius float dimless Minimum Arrhenius factor 5.0 max_arrhenius float ??? Maximum Arrhenius factor 151.0 min_slidingco float ??? Minimum sliding coefficient 0.0 max_slidingco float dimless Maximum sliding coefficient 20000.0 min_coarsen integer ??? Minimum coarsening factor 0 max_coarsen integer ??? Maximum coarsening factor 2 soft_begining integer dimless soft_begining, if 0 explore all parameters between min and max, otherwise, only explore from this iteration while keeping mid-value for the first it. 500"},{"location":"modules/processes/read_output/","title":"Module <code>read_output</code>","text":"<p>This module enables the reading of a previously generated NetCDF output file, allowing IGM to operate as though these quantities were freshly computed. It is particularly useful for testing the postprocessing module in isolation.</p> <p>Contributors: G. Jouvet</p>"},{"location":"modules/processes/read_output/#config-structure","title":"Config Structure","text":"<pre><code>read_output:\n  input_file: output.nc\n  crop: False\n  xmin: -1.0e+20\n  xmax: 1.0e+20\n  ymin: -1.0e+20\n  ymax: 1.0e+20\n</code></pre>"},{"location":"modules/processes/read_output/#parameters","title":"Parameters","text":"Name Type Units Description Default Value input_file str dimless NetCDF input data file output.nc crop bool dimless Crop the data from NetCDF file with given top/down/left/right bounds False xmin float m X left coordinate for cropping the NetCDF data -1e+20 xmax float m X right coordinate for cropping the NetCDF data 1e+20 ymin float m Y bottom coordinate for cropping the NetCDF data -1e+20 ymax float m Y top coordinate for cropping the NetCDF data 1e+20"},{"location":"modules/processes/rockflow/","title":"Module <code>rockflow</code>","text":"<p>This module extends the ice flow beyond glaciated areas by assigning a constant speed and along-slope flow direction. It is designed to track rock-like particles (using the <code>particles</code> module) in both ice-free and ice-covered regions. Particles are either advected at a constant speed (controlled by the parameter <code>processes.rockflow.speed</code>) following the steepest gradient of the ice-free terrain in 2D or transported by ice flow in 3D.</p> <p>Contributors: G. Jouvet</p>"},{"location":"modules/processes/rockflow/#config-structure","title":"Config Structure","text":"<pre><code>rockflow:\n  flow_speed: 1.0\n</code></pre>"},{"location":"modules/processes/rockflow/#parameters","title":"Parameters","text":"Name Type Units Description Default Value flow_speed float m yr^{-1} Speed of rock flow along the slope 1.0"},{"location":"modules/processes/smb_accpdd/","title":"Module <code>smb_accpdd</code>","text":"<p>Module <code>smb_accpdd</code> implements a combined accumulation and temperature-index model [Hock, 2003]. In this model, surface accumulation equals solid precipitation when the temperature is below a threshold and decreases linearly to zero in a transition zone. Conversely, surface ablation is computed proportionally to the number of Positive Degree Days (PDD). The model also tracks snow layer depth and applies different PDD proportionality factors for snow and ice. </p> <p>The computation of PDD uses the expectation integration formulation [Calov and Greve, 2005]. Additionally, the computation of the snowpack and refreezing parameters is adapted from the PyPDD and PISM implementations.</p>"},{"location":"modules/processes/smb_accpdd/#input","title":"Input","text":"<ul> <li><code>state.precipitation</code> [Unit: \\(kg \\, m^{-2} \\, y^{-1}\\) water equivalent]</li> <li><code>state.air_temp</code> [Unit: \\(^{\\circ}C\\)]</li> </ul>"},{"location":"modules/processes/smb_accpdd/#output","title":"Output","text":"<ul> <li><code>state.smb</code> [Unit: \\(m \\, ice \\, eq. \\, y^{-1}\\)]</li> </ul>"},{"location":"modules/processes/smb_accpdd/#references","title":"References","text":"<ul> <li>Hock, R. (2003). Temperature index melt modelling in mountain areas. Journal of Hydrology.</li> <li>Calov, R., &amp; Greve, R. (2005). A semi-analytical solution for the positive degree-day model with stochastic temperature variations. Journal of Glaciology.</li> </ul> <p>Contributors: G. Jouvet</p> <p>Note: This implementation is a TensorFlow re-implementation inspired by the one used in the Aletsch 1880\u20132100 example. It has been adapted to closely align (though not strictly) with the Positive Degree Day model implemented in PyPDD [Seguinot, 2019], which is utilized in the Parallel Ice Sheet Model (PISM) [Khroulev and the PISM Authors, 2020].</p> <ul> <li> <p>Seguinot, J. (2019). PyPDD: A positive degree day model for glacier surface mass balance (v0.3.1). Zenodo. https://doi.org/10.5281/zenodo.3467639</p> </li> <li> <p>Khroulev, C., &amp; the PISM Authors. (2020). PISM, a Parallel Ice Sheet Model v1.2: User\u2019s Manual. www.pism-docs.org</p> </li> </ul>"},{"location":"modules/processes/smb_accpdd/#config-structure","title":"Config Structure","text":"<pre><code>smb_accpdd:\n  update_freq: 1\n  refreeze_factor: 0.6\n  thr_temp_snow: 0.0\n  thr_temp_rain: 2.0\n  melt_factor_snow: 1.095726596343\n  melt_factor_ice:  2.921937590248\n  shift_hydro_year: 0.75\n  ice_density: 910.0\n  wat_density: 1000.0\n  smb_maximum_accumulation: 6.0\n</code></pre>"},{"location":"modules/processes/smb_accpdd/#parameters","title":"Parameters","text":"Name Type Units Description Default Value update_freq float y Update the mass balance each X years 1 refreeze_factor float dimless Refreezing factor 0.6 thr_temp_snow float ^{\\circ}C Threshold temperature for solid precipitation 0.0 thr_temp_rain float ^{\\circ}C Threshold temperature for liquid precipitation 2.0 melt_factor_snow float m yr^{-1} K^{-1} Degree-day factor for snow (ice eq.) 1.095726596343 melt_factor_ice float m yr^{-1} K^{-1} Degree-day factor for ice (ice eq.) 2.921937590248 shift_hydro_year float dimless This serves to start Oct 1. the acc/melt computation 0.75 ice_density float kg m^{-3} Density of ice for conversion of SMB into ice equivalent 910.0 wat_density float kg m^{-3} Density of water 1000.0 smb_maximum_accumulation float m yr^{-1} Maximum accumulation rate (m yr^{-1}) 6.0"},{"location":"modules/processes/smb_oggm/","title":"Module <code>smb_oggm</code>","text":"<p>Module <code>smb_oggm</code> implements the monthly temperature index model calibrated on geodetic mass balance (MB) data (Hugonnet, 2021) by OGGM. The yearly surface mass balance is computed with:</p> \\[SMB = \\frac{\\rho_w}{\\rho_i}  \\sum_{i=1}^{12} \\left( P_i^{sol} - d_f \\max \\{ T_i - T_{melt}, 0 \\} \\right),\\] <p>where \\(P_i^{sol}\\) is the monthly solid precipitation, \\(T_i\\) is the monthly temperature, and \\(T_{melt}\\) is the air temperature above which ice melt is assumed to occur (parameter <code>temp_melt</code>). The parameter \\(d_f\\) is the melt factor (parameter <code>melt_f</code>), and \\(\\frac{\\rho_w}{\\rho_i}\\) is the ratio of water to ice density. Solid precipitation \\(P_i^{sol}\\) is computed from precipitation and temperature such that it equals precipitation when the temperature is lower than a certain threshold (parameter <code>temp_all_solid</code>), zero above another threshold (parameter <code>temp_all_liq</code>), with a linear transition between the two. Module <code>oggm_shop</code> provides all calibrated parameters [1].</p> <p>Contributors: Guillaume Jouvet, Fabien Maussion</p> <p>Reference: Maussion, Fabien, et al. \"The open global glacier model (OGGM) v1. 1.\" Geoscientific Model Development 12.3 (2019): 909-931.</p>"},{"location":"modules/processes/smb_oggm/#config-structure","title":"Config Structure","text":"<pre><code>smb_oggm:\n  update_freq: 1\n  ice_density: 910.0\n  wat_density: 1000.0\n  melt_enhancer: 1\n</code></pre>"},{"location":"modules/processes/smb_oggm/#parameters","title":"Parameters","text":"Name Type Units Description Default Value update_freq float y Update the mass balance each X years 1 ice_density float kg m^{-3} Density of ice for conversion of SMB into ice equivalent 910.0 wat_density float kg m^{-3} Density of water 1000.0 melt_enhancer float dimless Melt enhancer factor 1"},{"location":"modules/processes/smb_simple/","title":"Module <code>smb_simple</code>","text":"<p>This IGM module models a simple surface mass balance (SMB) parametrized by time-evolving equilibrium line altitude (ELA) \\(z_{ELA}\\), ablation gradient \\(\\beta_{abl}\\), accumulation gradient \\(\\beta_{acc}\\), and maximum accumulation \\(m_{acc}\\) parameters:</p> \\[SMB(z) =  \\begin{cases}  \\min(\\beta_{acc} \\cdot (z - z_{ELA}), m_{acc}) &amp; \\text{if } z &gt; z_{ELA}, \\\\ \\beta_{abl} \\cdot (z - z_{ELA}) &amp; \\text{otherwise}. \\end{cases}\\] <p>These parameters can be provided in a file (specified by the <code>file</code> parameter) with the following format:</p> <pre><code>time   gradabl  gradacc    ela   accmax\n1900     0.009    0.005   2800      2.0\n2000     0.009    0.005   2900      2.0\n2100     0.009    0.005   3300      2.0\n</code></pre> <p>Alternatively, they can be directly specified in the configuration file <code>params.yaml</code> as follows:</p> <pre><code>smb_simple:\n  array: \n    - [\"time\", \"gradabl\", \"gradacc\", \"ela\", \"accmax\"]\n    - [ 1900,      0.009,     0.005,  2800,      2.0]\n    - [ 2000,      0.009,     0.005,  2900,      2.0]\n    - [ 2100,      0.009,     0.005,  3300,      2.0]\n</code></pre> <p>If the <code>array</code> parameter is set to an empty list <code>[]</code>, the module will read the data from the file specified by the <code>file</code> parameter. Otherwise, it will use the provided <code>array</code> (a list of lists).</p> <p>The module computes the surface mass balance at a frequency defined by the <code>update_freq</code> parameter (default is 1 year) and interpolates the four parameters linearly over time.</p> <p>If an \"icemask\" field is provided as input, the module will assign a negative surface mass balance (-10 m/y) to areas where a positive surface mass balance would otherwise occur outside the mask. This prevents overflow into neighboring catchments.</p> <p>Contributors: G. Jouvet</p>"},{"location":"modules/processes/smb_simple/#config-structure","title":"Config Structure","text":"<pre><code>smb_simple:\n  update_freq: 1.0\n  file: param.txt\n  array: []\n</code></pre>"},{"location":"modules/processes/smb_simple/#parameters","title":"Parameters","text":"Name Type Units Description Default Value update_freq float y Update the mass balance each X years 1.0 file string dimless Name of the input file for the simple mass balance model (time, gradabl, gradacc, ela, accmax) param.txt array list dimless Time dependent parameters for simple mass balance model (time, gradabl, gradacc, ela, accmax) []"},{"location":"modules/processes/texture/","title":"Module <code>texture</code>","text":"<p>This modules allows you to calculate ...</p> <p>Contributors: Brandon Finley</p>"},{"location":"modules/processes/texture/#config-structure","title":"Config Structure","text":"<pre><code>texture:\n  format: png\n  model_path: \"\"\n  verbosity: 30\n  divide_by_density: 1\n  resolution: -1\n</code></pre>"},{"location":"modules/processes/texture/#parameters","title":"Parameters","text":"Name Type Units Description Default Value format string dimless Format of the texture image (png, tif, or tiff) png model_path file path dimless Name of the folder for the texture model (tf format) verbosity integer dimless Python Logger verbosity level (10=DEBUG, 20=INFO, 30=WARNING, 40=ERROR, 50=CRITICAL) 30 divide_by_density boolean dimless This parameter solves an incompatibility (this option will be removed in the future) 1 resolution integer dimless This parameter solves an incompatibility (this option will be removed in the future) -1"},{"location":"modules/processes/texture/#example-usage","title":"Example Usage","text":""},{"location":"modules/processes/thk/","title":"Module <code>thk</code>","text":"<p>This IGM module solves the mass conservation equation for ice to update the thickness based on ice flow (computed by the <code>iceflow</code> module) and surface mass balance (provided by any module that updates <code>smb</code>). The equation is solved using an explicit first-order upwind finite-volume scheme on the 2D working grid. This scheme allows ice mass to move between cells (where thickness and velocities are defined) using edge-defined fluxes (calculated from depth-averaged velocities and ice thickness in the upwind direction). </p> <p>The scheme is mass-conservative and parallelizable due to its fully explicit nature. However, it is subject to a CFL condition, meaning the time step (defined in the <code>time</code> module) is constrained by the parameter <code>processes.time.cfl</code>. This parameter represents the maximum number of cells crossed in one iteration and cannot exceed one. For more details, refer to the documentation of the <code>time</code> module. Additional information about the scheme can be found in the following paper:</p> <p>Reference: Jouvet, G., Cordonnier, G., Kim, B., L\u00fcthi, M., Vieli, A., &amp; Aschwanden, A. (2022). Deep learning speeds up ice flow modelling by several orders of magnitude. Journal of Glaciology, 68(270), 651-664.</p> <p>Contributors: Guillaume Cordonnier, Guillaume Jouvet</p>"},{"location":"modules/processes/thk/#config-structure","title":"Config Structure","text":"<pre><code>thk:\n  slope_type: superbee\n  ratio_density: 0.910\n  default_sealevel: 0.0\n</code></pre>"},{"location":"modules/processes/thk/#parameters","title":"Parameters","text":"Name Type Units Description Default Value slope_type str dimless Type of slope limiter for the ice thickness equation (godunov or superbee) superbee ratio_density float dimless Density of ice divided by density of water 0.91 default_sealevel float m Default sea level if not provided by the user 0.0"},{"location":"modules/processes/time/","title":"Module <code>time</code>","text":"<p>This IGM module computes the time step such that: i) It satisfies the CFL condition (controlled by the parameter <code>processes.time.cfl</code>). ii) It is lower than a given maximum time step (controlled by the parameter <code>processes.time.step_max</code>). iii) It aligns exactly with specified saving times (controlled by the parameter <code>processes.time.save</code>).  </p> <p>The module also updates the current simulation time \\(t\\) in addition to determining the time step.</p> <p>For stability reasons related to the transport scheme for ice thickness evolution, the time step must adhere to the CFL condition. This condition is governed by the parameter <code>processes.time.cfl</code>, which specifies the maximum number of cells that can be crossed in one iteration (this parameter cannot exceed 1). By default, <code>processes.time.cfl</code> is set to 0.3. Additionally, the time step is constrained by a user-defined maximum time step, <code>processes.time.step_max</code>, and must align with the saving frequency defined by <code>processes.time.save</code> (default: 1 year).</p> <p>Key parameters of this module include: - <code>processes.time.start</code>: Defines the simulation start time. - <code>processes.time.end</code>: Defines the simulation end time. - <code>processes.time.save</code>: Specifies the frequency at which results are saved (default: 10 years).</p> <p>Further details on the time step stability conditions can be found in the following paper:   </p> <p>Reference: Jouvet, G., Cordonnier, G., Kim, B., L\u00fcthi, M., Vieli, A., &amp; Aschwanden, A. (2022). Deep learning speeds up ice flow modelling by several orders of magnitude. Journal of Glaciology, 68(270), 651-664.</p> <p>Contributors: G. Jouvet</p>"},{"location":"modules/processes/time/#config-structure","title":"Config Structure","text":"<pre><code>time:\n  start: 2000.0\n  end: 2100.0\n  save: 10.0\n  cfl: 0.3\n  step_max: 1.0\n</code></pre>"},{"location":"modules/processes/time/#parameters","title":"Parameters","text":"Name Type Units Description Default Value start float y Start time of the simulation 2000.0 end float y End time of the simulation 2100.0 save float y Frequency of saving the simulation results 10.0 cfl float dimless CFL condition for time stepping 0.3 step_max float y Maximum time step allowed 1.0"},{"location":"modules/processes/vert_flow/","title":"Module <code>vert_flow</code>","text":"<p>This IGM module computes the vertical component of the velocity (<code>state.W</code>) from the horizontal components (<code>state.U</code> and <code>state.V</code>). These horizontal components are derived from an emulation of the Blatter-Pattyn model in the <code>iceflow</code> module. The computation is performed by integrating the incompressibility condition layer-wise. This module is typically used before invoking the <code>particle</code> module for 3D particle trajectory integration or the <code>enthalpy</code> module for computing 3D advection-diffusion of enthalpy.</p> <p>Contributors: Guillaume Jouvet, Claire-Mathile St\u00fccki</p>"},{"location":"modules/processes/vert_flow/#config-structure","title":"Config Structure","text":"<pre><code>vert_flow:\n  version: 2\n  method: kinematic\n</code></pre>"},{"location":"modules/processes/vert_flow/#parameters","title":"Parameters","text":"Name Type Units Description Default Value version integer dimless Version of the vert_flow method (1 original, 2 improved by claire-mathilde) 2 method string dimless Method to retrieve the vertical velocity (kinematic, incompressibility) kinematic"}]}